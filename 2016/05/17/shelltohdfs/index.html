<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="HMmFpsFIIdXroD_bySStaKsE_InwsQEEVxA2Cne-P4o" name="google-site-verification"><meta content="telephone=no" name="format-detection"><meta name="description" content="spark8,Spark8,Spark2.0,spark2.0"><title>Shell编程——使用HDFS Shell上传文件 | Spark8</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.1"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.1.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.2/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/2.2.3/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Shell编程——使用HDFS Shell上传文件</h1><a id="logo" href="/.">Spark8</a><p class="description">Do something you want,Be someone you like!</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Shell编程——使用HDFS Shell上传文件</h1><div class="post-meta">May 17, 2016<span> | </span><span class="category"><a href="/categories/Linux/">Linux</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="2016/05/17/shelltohdfs/" href="/2016/05/17/shelltohdfs/#disqus_thread" class="disqus-comment-count"></a><div class="post-content"><a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>点击流日志每天都10G，需要上传数据仓库（Hadoop HDFS）上</p>
<h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>一般上传文件都是在凌晨24点操作，由于很多种类的业务数据都要在晚上进行传输，为了减轻服务器的压力，避开高峰期。需要伪实时的上传，即当文件有10G的时候，就上传一个。<br>一、    正常上传到hdfs的需求</p>
<ol>
<li>上传到hdfs的文件需要校验完整性。MD5</li>
<li>一个文件在上传过程中另一个进程不能再次上传该文件  <em>COPY</em></li>
<li>上传到hdfs的文件名不能相同   时间戳</li>
<li>定时上传文件到hdfs  PUT<br>二、    异常上传的需求</li>
<li><p>文件上传到hdfs过程中失败，需要有重试机制，<br> 若能再次连上需要保证数据能再次上传且数据不重复，也不丢失，<br> HDFS文件列表和本地待上传的文件列表进行比对<br> grep -diff<br> 若超过重试限制后需要发短信通知<br> java -jar </p>
</li>
<li><p>ftp服务连不上，需要有重试机制，超过重试限制需要发短信通知</p>
</li>
<li>运行脚本的服务器挂了，需要有备份服务器启动，并发短信通知<h2 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h2><h3 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h3></li>
<li>HDFS SHELL:  hadoop fs  –put   xxxx.tar  /data    还可以使用 Java Api<br>  满足上传一个文件，不能满足定时、周期性传入。</li>
<li><p>Linux crontab<br>“crontab -e <em>/5 </em> <em> </em> * $home/bin/command.sh ” //五分钟执行一次<br>系统会自动执行脚本，每五分钟一次，执行时判断文件是否等于10G，如果等于10G就可以上传。</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>以下为正常上传部分代码，异常上传部分未实现，只提供如上思路</p>
<pre><code>!/bin/bash
set java env
export JAVA_HOME=/export/servers/jdk
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH

set hadoop env
export HADOOP_HOME=/export/servers/hadoop
export PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH
</code></pre><p>日志文件存放的目录</p>
<pre><code>log_src_dir=/export/software/

s=du -k $log_src_dir |awk &apos;{print $1}&apos;
if [ $s -lt 1024000000 ]
then
exit 1
else
continue
fi
</code></pre><p>待上传文件存放的目录</p>
<pre><code>log_appending_dir=/export/data/click_log/
</code></pre><p>日志文件上传到hdfs的根路径</p>
<pre><code>hdfs_root_dir=/data/clickLog/20151226/
</code></pre><p>读取日志文件的目录，判断是否有需要上传的文件</p>
<pre><code>ls $log_src_dir | while read fileName
do
if [ &quot;hadoop.log1&quot; = &quot;$fileName&quot; ];then
date=date +%Y_%m_%d_%H_%M_%S
mv $log_src_dir$fileName $log_appending_dir&quot;xxxxx_click_log_&quot;$date
echo $log_appending_dir&quot;xxxxx_click_log_&quot;$date &gt;&gt; /export/data/click_log/willDoing.$date
fi

done
</code></pre><p>过滤掉正在copy和已经copy的文件</p>
<pre><code>ls $log_appending_dir | grep will |grep -v &quot;_COPY_&quot; | grep -v &quot;_DONE_&quot; | while read line
do
</code></pre><p>对拿到的文件重命名</p>
<pre><code>mv $log_appending_dir$line $log_appending_dir$line&quot;_COPY_&quot;
cat $log_appending_dir$line&quot;_COPY_&quot; |while read line
do
hadoop fs -put $line $hdfs_root_dir
done    
mv $log_appending_dir$line&quot;_COPY_&quot;  $log_appending_dir$line&quot;_DONE_&quot;
done
</code></pre><p>重试核心代码</p>
<pre><code>function failOver(){
    echo &quot;重试机制启动....&quot;
    #转钟逻辑处理
    if [ &quot;00:00&quot; = $tag ] || [ &quot;00:01&quot; = $tag ]
    then
    yesterday_dt=date --date=&apos;yesterday&apos; +%Y-%m-%d
    yesterday_ftp_date=date --date=&apos;yesterday&apos; +%Y%m%d
    put_local_file_to_hdfs $yesterday_dt $yesterday_ftp_date
    fi
    #半小时逻辑处理
    if [ &quot;40&quot; = $tagM ] || [ &quot;41&quot; = $tagM ]
    then
            today_dt=date +%Y-%m-%d
            today_ftp_date=date +%Y%m%d
            put_local_file_to_hdfs $today_dt $today_ftp_date
    fi
    echo &quot;重试机制执行完毕...&quot;
}

function put_local_file_to_hdfs(){
</code></pre></li>
</ol>
<pre><code>    diffFile=$log_dir&quot;differ_file&quot;$executeTime.log
    hdfs_file_list=$log_dir&quot;hdfs_file_list&quot;$executeTime.log
    local_file_list=$log_dir&quot;local_file_list&quot;$executeTime.log

    hadoop fs -ls /apps/hive/warehouse/stage.db/$hdfs_table/dt=$1/ | awk &apos;{print $8}&apos;|while read line
    do
            file_name=${line##*/}
            suffix=${file_name##*.}
            #如果文件正在上传中，视为已经上传成功
            if [ &quot;_COPYING_&quot; = &quot;$suffix&quot; ]
            then
                    echo &quot;此文件正在上传：&quot;$line
                    file_name=${file_name%.*}
            fi
            echo $file_name &gt;&gt; $hdfs_file_list
    done

    ls $tmp_Dir$2 |grep $file_prefix &gt;$local_file_list

    grep -vxFf $hdfs_file_list $local_file_list &gt; $diffFile

    rm $hdfs_file_list
    rm $local_file_list

    file_size=cat $diffFile |wc -l
    echo &quot;当前重试的文件个数：&quot;$file_size
    if (( &quot;$file_size&quot; &gt;= &quot;1&quot; ))
    then
            #如果重试的文件大于，触发短信告警信息
            #source ~/.base_profile
            /soft/java/bin/java -jar /export/server/real_platform/sendmsg/sendmsg.jar &quot;15652306418,18211153576&quot; $hostname&quot;机器下有 &quot;$file_size&quot; 个文件正在重试上传，小偷程序遇到问题了，请查看！&quot;
            echo &quot;发送短信成功！&quot;
    fi

    if [ -f $diffFile ]
    then 
            cat $diffFile |while read line
                            do
                            tarFile=$tmp_Dir$2&quot;/&quot;$line
                            echo &quot;开始上传文件：&quot;$tarFile
                            echo &quot;hadoop命令：hadoop fs -put &quot;$tarFile &quot;/apps/hive/warehouse/stage.db/&quot;$hdfs_table&quot;/dt=&quot;$1&quot;/&quot;
                            hadoop fs -put $tarFile /apps/hive/warehouse/stage.db/$hdfs_table/dt=$1/
                            echo &quot;上传文件结束：&quot;$tarFile
                            done
            rm $diffFile 
    fi
}
</code></pre></div><script type="text/javascript" src="/js/share.js?v=0.0.1" async></script><a data-url="http://spark8.tech/2016/05/17/shelltohdfs/" data-id="cipruborz000jfrw2vf8dzjgq" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Linux/">Linux</a><a href="/tags/Shell脚本/">Shell脚本</a></div><div class="post-nav"><a href="/2016/05/17/rizhijiankong1/" class="pre">日志监控告警系统（一）</a><a href="/2016/05/16/Linuxline/" class="next">Linux——网络连接三种方式</a></div><div id="disqus_thread"><script>var disqus_shortname = 'sparkliwei';
var disqus_identifier = '2016/05/17/shelltohdfs/';
var disqus_title = 'Shell编程——使用HDFS Shell上传文件';
var disqus_url = 'http://spark8.tech/2016/05/17/shelltohdfs/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//sparkliwei.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/RPC/">RPC</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/redis/">redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/storm/">storm</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/RPC/" style="font-size: 15px;">RPC</a> <a href="/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/tags/storm/" style="font-size: 15px;">storm</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/flume/" style="font-size: 15px;">flume</a> <a href="/tags/Shell脚本/" style="font-size: 15px;">Shell脚本</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/05/18/rizhijiankong2/">日志监控告警系统（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/17/rizhijiankong1/">日志监控告警系统（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/17/shelltohdfs/">Shell编程——使用HDFS Shell上传文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/16/Linuxline/">Linux——网络连接三种方式</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/15/shellautodeploy/">Shell编程——自动化软件部署脚本</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/12/easyrpc2/">轻量级RPC框架（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/12/easyrpc/">轻量级RPC框架（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/14/redis_sortedSet/">Redis应用-LOL盒子英雄数据排行榜</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//sparkliwei.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://spark8.tech/" title="spark8" target="_blank">spark8</a><ul></ul><a href="https://github.com/sparkLiwei" title="github" target="_blank">github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Spark8.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.1" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.1"><script type="text/javascript" src="/js/search.js?v=0.0.1"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-79665129-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?561e14522c8f418b3f35a30cba92c642";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  var bp = document.createElement('script');
  var curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
     bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else {
     bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  s.parentNode.insertBefore(bp, s);
  })();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.1"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.1"></script></div></body></html>