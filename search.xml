<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Hadoop2.x——左连接]]></title>
      <url>http://spark8.tech/2016/06/22/hadoopzlj/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>有两张表，关联字段为pid，订单表中没有商品名称，需要对数据进行补全，使用MapReduce思想，进行编程。</p>
<blockquote>
<p> 订单数据表t_order：<br>  id    date        pid      amount<br> 1001    20150710    P0001    2<br> 1002    20150710    P0001    3<br> 1002    20150710    P0002    3</p>
<p> 商品信息表t_product<br>  id        pname    category_id    price<br>  P0001    小米5    1000        2<br>  P0002    锤子T1    1000        3</p>
<p>订单表中没有商品名称，需要对数据进行补全<br>1001    20150710    P0001    小米5     2<br>1002    20150710    P0001    小米5     3<br>1002    20150710    P0002    锤子T3  3</p>
</blockquote>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>数据输入：order.txt、pd.txt<br>1、Map 阶段</p>
<blockquote>
<p>KEYOUT  pid<br>VALUEOUT OrderAllInfo writealbe(write,read)<br>输入两个文件的数据<br>VALUEIN 1001      20150710    P0001    2<br>VALUEIN 1001      20150710    P0001    2<br>VALUEIN 1001      20150710    P0001    2<br>来自不同的文件，文件名不一样<br>VALUEIN   P0001    小米5    1000    2<br>VALUEIN   P0001    小米5    1000    2<br>来自不同的文件，文件名不一样<br>能够在OrderAllInfo中加入一个tag标志，当文件名等于order，<br>tag取值为1 当文件名等于product时，tag取值为0<br>2、Reduce 阶段<br>KEYIN   pid<br>VALUEiN        iterable\<orderallinfo,orderallinfo,orderallinfo,orderallinfo\></orderallinfo,orderallinfo,orderallinfo,orderallinfo\></p>
<p>OrderAllInfo tag=0 记录，商品记录只有一条      商品表<br>OrderAllInfo tag=1                                                     订单表<br>OrderAllInfo tag=1                          订单表<br>过程<br>OrderAllInfo tag=1 +  OrderAllInfo tag=0   = 新的 OrderAllInfo<br>OrderAllInfo tag=1 +  OrderAllInfo tag=0   = 新的 OrderAllInfo<br>得到<br>1001     20150710    P0001    小米5     2<br>1002    20150710    P0001    小米5     3<br>1002    20150710    P0002    锤子T3     3</p>
<h2 id="代码开发"><a href="#代码开发" class="headerlink" title="代码开发"></a>代码开发</h2><p>源码请参考<a href="www.baidu.com">github链接</a>,以下为核心实现。<br>1.驱动类<br><img src="/images/zljqdl.png" alt="" title="zljqdl"><br>2.Map类<br><img src="/images/zljmapl.png" alt="" title="zljmap"><br>3.Reduce类<br><img src="/images/zljreducel.png" alt="" title="zljreducel"></p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop2.x——自定义输出格式重写outputformat]]></title>
      <url>http://spark8.tech/2016/06/22/zidingyiot/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>现有一些原始日志需要做增强解析处理，流程<br>1、从原始日志文件中读取数据<br>2、根据日志中的一个URL字段到外部知识库中获取信息增强到原始日志<br>3、如果成功增强，则输出到增强结果目录；如果增强失败，则抽取原始数据中URL字段输出到待爬清单目录</p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>程序的关键点是要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现<br>实现要点：<br>1、在mapreduce中访问外部资源<br>2、自定义outputformat，改写其中的recordwriter，改写具体输出数据的方法write()</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>代码实现如下：<br>数据库获取数据的工具</p>
<pre><code>public class DBLoader {
    public static void dbLoader(HashMap&lt;String, String&gt; ruleMap) throws Exception {

        Connection conn = null;
        Statement st = null;
        ResultSet res = null;

        try {
            Class.forName(&quot;com.mysql.jdbc.Driver&quot;);
            conn = DriverManager.getConnection(&quot;jdbc:mysql://hdp-node-01:3306/urldb&quot;, &quot;root&quot;, &quot;root&quot;);
            st = conn.createStatement();
            res = st.executeQuery(&quot;select url,content from url_rule&quot;);
            while (res.next()) {
                ruleMap.put(res.getString(1), res.getString(2));
            }
        } finally 
            try{
                if(res!=null){
                    res.close();
                if(st!=null)
                    st.close();
                if(conn!=null)
                    conn.close();
            }catch(Exception e){
                e.printStackTrace();    }
        }
    }
}
</code></pre><p>FlowBean</p>
<pre><code>public class FlowBean implements WritableComparable&lt;FlowBean&gt; {

    private String url;
    private long upflow;

    public FlowBean() {
    }

    public void set(String url, long upflow) {

        this.url = url;
        this.upflow = upflow;

    }

    public String getUrl() {
        return url;
    }

    public void setUrl(String url) {
        this.url = url;
    }

    public long getUpflow() {
        return upflow;
    }

    public void setUpflow(long upflow) {
        this.upflow = upflow;
    }

    @Override
    public void readFields(DataInput in) throws IOException {

        url = in.readUTF();
        upflow = in.readLong();

    }

    @Override
    public void write(DataOutput out) throws IOException 
        out.writeUTF(url);
        out.writeLong(upflow);
    @Override
    public int compareTo(FlowBean o) {
        return this.upflow &gt; o.getUpflow() ? -1 : 1;
    }    
    @Override
    public String toString() 

        return  this.url + &quot;\t&quot; + this.upflow;

    public static void main(String[] args) 
        String ss = &quot;1374609503.88    1374609503.92    1374609503.92    1374609504.08    110    5    8613674936776    460003460238162    3595900488375164    2    460    0    14306            20193    10.184.41.121    123.125.114.195    45926    80    6    cmnet    1    221.177.156.14    221.177.217.145    221.177.156.14    221.177.217.155    mobads-logs.baidu.com    http://mobads-logs.baidu.com/dc?type=20&amp;st=pv&amp;rnd=0.9601120455190539        Mozilla/5.0 (Linux; U; Android 2.3.4; zh-CN; ST18i Build/4.0.2.A.0.62) AppleWebKit/534.31 (KHTML, like Gecko) UCBrowser/9.1.1.309 U3/0.8.0 Mobile Safari/534.31    GET    200    1317    291    7    2    0    0    7    2    0    0    0    0    http://mobads-logs.baidu.com/dc?type=20&amp;st=pv&amp;rnd=0.9601120455190539    5903902610650624011    5903902862210048011    5966330&quot;;
        String[] fields = StringUtils.split(ss,&quot;\t&quot;);
        System.out.println(fields[26]);
        System.out.println(fields[30]);    
}
</code></pre><p>开发mapreduce处理流程</p>
<pre><code>/**
 * 日志增强：从原始日志中获取url，然后去知识库查找内容信息标签，追加到原始日志中
 * 如果查不到的url，就输出到带爬清单中
 * 
 * @author
 * 
 */
public class LogEnhancer extends Configured implements Tool{

    static class LogEnhancerMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; {

        HashMap&lt;String, String&gt; knowledgeMap = new HashMap&lt;String, String&gt;();

        /**
         * maptask
         */
        @Override
        protected void setup(Context context) throws IOException, InterruptedException {

            try {
                DBLoader.dbLoader(knowledgeMap);
            } catch (Exception e) {
                // TODO Auto-generated catch block
                e.printStackTrace();
            }

        }

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

            String line = value.toString();

            String[] fields = StringUtils.split(line, &quot;\t&quot;);

            try {
                String url = fields[26];

                // 从规则库中匹配标签信息
                String content = knowledgeMap.get(url);

                // 判断规则库匹配结果
                String result = &quot;&quot;;
                if (null == content) {
                    // 如果匹配失败，则将这条url到带爬清单文件中
                    result = url + &quot;\t&quot; + &quot;tocrawl\n&quot;;
                } else {
                    // 如果匹配成功，则将这条“原始日志+内容标签”输出到增强日志文件中
                    result = line + &quot;\t&quot; + content + &quot;\n&quot;;
                }

                context.write(new Text(result), NullWritable.get());
            } catch (Exception e) {

            }
        }

    }

    public static void main(String[] args) throws Exception {

        Configuration conf = new Configuration();

        Job job = Job.getInstance(conf);

        job.setJarByClass(LogEnhancer.class);

        job.setMapperClass(LogEnhancerMapper.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(NullWritable.class);

        // 要控制不同的内容写往不同的目标路径，可以采用自定义outputformat的方法
        job.setOutputFormatClass(LogEnhancerOutputFormat.class);

        FileInputFormat.setInputPaths(job, new Path(args[0]));

        //尽管我们用的是自定义outputformat，但是它是继承制fileoutputformat
        //在fileoutputformat中，必须输出一个success文件，所以在此还需要设置输出path
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        //不需要reducer
        job.setNumReduceTasks(0);


        job.waitForCompletion(true);
        System.exit(0);

    }
    @Override
    public int run(String[] args) throws Exception {
        // TODO Auto-generated method stub
        return 0;
    }
}
</code></pre><p>自定义outputformat</p>
<pre><code>public class LogEnhancerOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;{


    @Override
    public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException {


        FileSystem fs = FileSystem.get(context.getConfiguration());
        Path enhancePath = new Path(&quot;hdfs://hadoop01:9000/wordcount/enhancelog/enhanced.log&quot;);
        Path toCrawlPath = new Path(&quot;hdfs://hadoop01:9000/wordcount/tocrawl/tocrawl.log&quot;);

        FSDataOutputStream enhanceOut = fs.create(enhancePath);
        FSDataOutputStream toCrawlOut = fs.create(toCrawlPath);


        return new MyRecordWriter(enhanceOut,toCrawlOut);
    }



    static class MyRecordWriter extends RecordWriter&lt;Text, NullWritable&gt;{

        FSDataOutputStream enhanceOut = null;
        FSDataOutputStream toCrawlOut = null;

        public MyRecordWriter(FSDataOutputStream enhanceOut, FSDataOutputStream toCrawlOut) {
            this.enhanceOut = enhanceOut;
            this.toCrawlOut = toCrawlOut;
        }

        @Override
        public void write(Text key, NullWritable value) throws IOException, InterruptedException {

            //判断数据的一些标记，根据不同标记写往不同文件
            //增日志写往enhanceOut，待爬清单写往toCrawlOut
            if(key.toString().contains(&quot;tocrawl&quot;)){
                toCrawlOut.write(key.toString().getBytes());
            }else{
                enhanceOut.write(key.toString().getBytes());
            }

        }

        @Override
        public void close(TaskAttemptContext context) throws IOException, InterruptedException {

            if(toCrawlOut!=null){
                toCrawlOut.close();
            }
            if(enhanceOut!=null){
                enhanceOut.close();
            }

        }


    }


}
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop2.x——实现HA]]></title>
      <url>http://spark8.tech/2016/06/22/hadoopha/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="HA的运作机制"><a href="#HA的运作机制" class="headerlink" title="HA的运作机制"></a>HA的运作机制</h2><p>(1)  hadoop-HA集群运作机制介绍<br>所谓HA，即高可用（7*24小时不中断服务）<br>实现高可用最关键的是消除单点故障<br>hadoop-ha严格来说应该分成各个组件的HA机制——HDFS的HA、YARN的HA<br>(2) HDFS的HA机制详解<br>通过双namenode消除单点故障<br>双namenode协调工作的要点：<br>A、元数据管理方式需要改变：<br>内存中各自保存一份元数据<br>Edits日志只能有一份，只有Active状态的namenode节点可以做写操作<br>两个namenode都可以读取edits<br>共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现）<br>B、需要一个状态管理功能模块<br>实现了一个zkfailover，常驻在每一个namenode所在的节点<br>每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识<br>当需要进行状态切换时，由zkfailover来负责切换<br>切换时需要防止brain split现象的发生</p>
<h2 id="HDFS-HA图解"><a href="#HDFS-HA图解" class="headerlink" title="HDFS-HA图解"></a>HDFS-HA图解</h2><p><img src="/images/hdfsha.png" alt="" title="hdfsha"></p>
<h2 id="HA集群的安装部署"><a href="#HA集群的安装部署" class="headerlink" title="HA集群的安装部署"></a>HA集群的安装部署</h2><h3 id="集群节点规划"><a href="#集群节点规划" class="headerlink" title="集群节点规划"></a>集群节点规划</h3><p>10节点<br><img src="/images/10jiedian.png" alt="" title="10jiedian"><br>8节点<br><img src="/images/8jiedian.png" alt="" title="8jiedian"></p>
<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>环境准备<br>a/linux系统准备<br>ip地址配置<br>hostname配置<br>hosts映射配置<br>防火墙关闭<br>init启动级别修改3<br>sudoers加入hadoop用户<br>ssh免密登陆配置</p>
<p>b/java环境的配置<br>上传jdk，解压，修改/etc/profile</p>
<p> c/zookeeper集群的部署</p>
<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>core-site.xml</p>
<pre><code>&lt;configuration&gt;
&lt;!-- 指定hdfs的nameservice为ns1 --&gt;
&lt;property&gt;
&lt;name&gt;fs.defaultFS&lt;/name&gt;
&lt;value&gt;hdfs://ns1/&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 指定hadoop临时目录 --&gt;
&lt;property&gt;
&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
&lt;value&gt;/home/hadoop/app/hadoop-2.4.1/tmp&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 指定zookeeper地址 --&gt;
&lt;property&gt;
&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;
&lt;value&gt;weekend05:2181,weekend06:2181,weekend07:2181&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre><p>hdfs-site.xml</p>
<pre><code>&lt;configuration&gt;
&lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.nameservices&lt;/name&gt;
&lt;value&gt;ns1&lt;/value&gt;
&lt;/property&gt;
&lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt;
&lt;value&gt;nn1,nn2&lt;/value&gt;
&lt;/property&gt;
&lt;!-- nn1的RPC通信地址 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.rpc-address.ns1.nn1&lt;/name&gt;
&lt;value&gt;weekend01:9000&lt;/value&gt;
&lt;/property&gt;
&lt;!-- nn1的http通信地址 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.http-address.ns1.nn1&lt;/name&gt;
&lt;value&gt;weekend01:50070&lt;/value&gt;
&lt;/property&gt;
&lt;!-- nn2的RPC通信地址 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.rpc-address.ns1.nn2&lt;/name&gt;
&lt;value&gt;weekend02:9000&lt;/value&gt;
&lt;/property&gt;
&lt;!-- nn2的http通信地址 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.http-address.ns1.nn2&lt;/name&gt;
&lt;value&gt;weekend02:50070&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 指定NameNode的edits元数据在JournalNode上的存放位置 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;
&lt;value&gt;qjournal://weekend05:8485;weekend06:8485;weekend07:8485/ns1&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;
&lt;value&gt;/home/hadoop/app/hadoop-2.4.1/journaldata&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 开启NameNode失败自动切换 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 配置失败自动切换实现方式 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;
&lt;property&gt;
&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;
&lt;value&gt;
sshfence
shell(/bin/true)
&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;
&lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 配置sshfence隔离机制超时时间 --&gt;
&lt;property&gt;
&lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;
&lt;value&gt;30000&lt;/value&gt;
&lt;/property&gt;
/configuration&gt;
</code></pre><h3 id="集群运维测试"><a href="#集群运维测试" class="headerlink" title="集群运维测试"></a>集群运维测试</h3><p>HA集群中两个namenode状态的管理命令</p>
<blockquote>
<p><a href="#">root@mini2 hadoop-2.6.4</a># bin/hdfs haadmin<br>Usage: DFSHAAdmin <a href="#">-ns \<nameserviceid\></nameserviceid\></a><br>[-transitionToActive \<serviceid\> <a href="#">–forceactive</a>]<br><a href="#">-transitionToStandby \<serviceid\></serviceid\></a><br>[-failover <a href="#">–forcefence</a> <a href="#">–forceactive</a> \<serviceid\> \<serviceid\>]<br><a href="#">-getServiceState \<serviceid\></serviceid\></a><br><a href="#">-checkHealth \<serviceid\></serviceid\></a><br><a href="#">-help \<command\></command\></a><br>示例： 切换nn2为active<br>bin/hdfs haadmin -transitionToActive nn2 –forcemanual</serviceid\></serviceid\></serviceid\></p>
<h4 id="Datanode动态上下线"><a href="#Datanode动态上下线" class="headerlink" title="Datanode动态上下线"></a>Datanode动态上下线</h4><p>Datanode动态上下线很简单，步骤如下：<br>a)    准备一台服务器，设置好环境<br>b)    部署hadoop的安装包，并同步集群配置<br>c)    联网上线，新datanode会自动加入集群<br>d)    如果是一次增加大批datanode，还应该做集群负载重均衡</p>
<h4 id="Namenode状态切换管理"><a href="#Namenode状态切换管理" class="headerlink" title="Namenode状态切换管理"></a>Namenode状态切换管理</h4><p>使用的命令上hdfs  haadmin<br>可用 hdfs  haadmin –help查看所有帮助信息<br><img src="/images/hahelp.png" alt="" title="hahelp"><br>可以看到，状态操作的命令示例：<br>查看namenode工作状态<br>    hdfs haadmin -getServiceState nn1<br>将standby状态namenode切换到active<br>    hdfs haadmin –transitionToActive nn1<br>将active状态namenode切换到standby<br>    hdfs haadmin –transitionToStandby nn2</p>
<h4 id="数据块的balance"><a href="#数据块的balance" class="headerlink" title="数据块的balance"></a>数据块的balance</h4><p>启动balancer的命令：<br>start-balancer.sh -threshold 8<br>运行之后，会有Balancer进程出现：<br><img src="/images/balanceha.png" alt="" title="balanceha"><br>上述命令设置了Threshold为8%，那么执行balancer命令的时候，首先统计所有DataNode的磁盘利用率的均值，然后判断如果某一个DataNode的磁盘利用率超过这个均值Threshold，那么将会把这个DataNode的block转移到磁盘利用率低的DataNode，这对于新节点的加入来说十分有用。Threshold的值为1到100之间，不显示的进行参数设置的话，默认是10。</p>
<h3 id="HA下hdfs-api变化"><a href="#HA下hdfs-api变化" class="headerlink" title="HA下hdfs-api变化"></a>HA下hdfs-api变化</h3><p>客户端需要nameservice的配置信息，其他不变</p>
</blockquote>
<pre><code>/**
如果访问的是一个ha机制的集群
则一定要把core-site.xml和hdfs-site.xml配置文件放在客户端程序的classpath下
以让客户端能够理解hdfs://ns1/中  “ns1”是一个ha机制中的namenode对——nameservice
以及知道ns1下具体的namenode通信地址
@author
 *
 */
public class UploadFile {
    public static void main(String[] args) throws Exception  {

    Configuration conf = new Configuration();
    conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://ns1/&quot;);

    FileSystem fs = FileSystem.get(new URI(&quot;hdfs://ns1/&quot;),conf,&quot;hadoop&quot;);

    fs.copyFromLocalFile(new Path(&quot;g:/eclipse-jee-luna-SR1-linux-gtk.tar.gz&quot;), new Path(&quot;hdfs://ns1/&quot;));
    fs.close(); 
    }
}
</code></pre><p>Federation下 mr程序运行的staging提交目录问题<br>        <property><br>          <name>yarn.app.mapreduce.am.staging-dir</name><br>          <value>/bi/tmp/hadoop-yarn/staging</value><br>          <description>The staging dir used while submitting jobs.<br>          </description><br>        </property></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop2.x——共同好友问题]]></title>
      <url>http://spark8.tech/2016/06/22/gongtonghaoyou/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>已知每人的好友，求两两之间的共同好友<br>    A:B,C,D,F,E,O<br>    B:A,C,E,K<br>    C:F,A,D,I<br>    D:A,E,F,L<br>    E:B,C,D,M,L<br>    F:A,B,C,D,E,O,M<br>    G:A,C,D,E,F<br>    H:A,C,D,E,O<br>    I:A,O<br>    J:B,O<br>    K:A,C,D<br>    L:D,E,F<br>    M:E,F,G<br>    O:A,H,I,J</p>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>step1：每一个人，都是哪些人的共同好友<br>A    I-K-C-B-G-F-H-O-D-<br>B    A-F-J-E-<br>C    A-E-B-H-F-G-K-<br>D    G-C-K-A-L-F-E-H-<br>E    G-M-L-H-A-F-B-D-<br>step2：第二个步骤的输入数据是上一个步骤的输出<br>把有共同好友的人，两两配对作为key，好友作为value发出去，以便在reduce中能拿到一个两两对的所有好友<br>a    b-d-f-g-h-i-k-o-     BD-A  BF-A  BG-A BH-A BI-A BK-A BO-A DF-A DG-A</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>Step1：</p>
<pre><code>public class CommonFriends {
    static class CommonFriendsMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; {
        // A:B,C,D,F,E,O B-A C-A D-A F-A E-A O-A
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            String[] split = line.split(&quot;:&quot;);
            String person = split[0];
            String[] friends = split[1].split(&quot;,&quot;);
            for (String f : friends) {
                context.write(new Text(f), new Text(person));
            }
        }
    }
    static class CommonFriendsReducer extends Reducer&lt;Text, Text, Text, Text&gt;{        
        //B {a,e,f,j}
        @Override
        protected void reduce(Text f, Iterable&lt;Text&gt; persons, Context context) throws IOException, InterruptedException {
            StringBuilder sb = new StringBuilder();
            for(Text p:persons){
                sb.append(p+&quot;-&quot;);
            }
            //B A-E-F-J
            context.write(f, new Text(sb.toString()));    
        }        
    }
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);
        job.setJarByClass(CommonFriends.class);
        job.setMapperClass(CommonFriendsMapper.class);
        job.setReducerClass(CommonFriendsReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        FileInputFormat.setInputPaths(job, new Path(&quot;C:\\wordcount\\friends&quot;));
        FileOutputFormat.setOutputPath(job, new Path(&quot;c:/wordcount/friends-step1&quot;));
        job.waitForCompletion(true);
    }
}
</code></pre><p>step2：</p>
<pre><code>public class CommonFriendsStepTwo {
    static class CommonFriendsStepTwoMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; {
        // a    b-d-f-g-h-i-k-o-     BD-A  BF-A  BG-A BH-A BI-A BK-A BO-A DF-A DG-A
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            String[] split = line.split(&quot;\t&quot;);
            String friend = split[0];
            String[] persons = split[1].split(&quot;-&quot;);
            Arrays.sort(persons);
            for(int i=0;i&lt;persons.length-1;i++){
                for(int j=i+1;j&lt;persons.length;j++){
                    context.write(new Text(persons[i]+&quot;-&quot;+persons[j]), new Text(friend));
                }
            }
        }
    }
    static class CommonFriendsStepTwoReducer extends Reducer&lt;Text, Text, Text, Text&gt;{
        //BD-A BD-C BD-G
        @Override
        protected void reduce(Text pair, Iterable&lt;Text&gt; friends, Context context) throws IOException, InterruptedException {
            StringBuffer sb = new StringBuffer();
            for(Text f:friends){        
                sb.append(f+ &quot;,&quot;);
            }
            context.write(pair, new Text(sb.toString()));    
        }
    }
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);
        job.setJarByClass(CommonFriendsStepTwo.class);
        job.setMapperClass(CommonFriendsStepTwoMapper.class);
        job.setReducerClass(CommonFriendsStepTwoReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        FileInputFormat.setInputPaths(job, new Path(&quot;C:\\wordcount\\friends-step1&quot;));
        FileOutputFormat.setOutputPath(job, new Path(&quot;C:\\wordcount\\friends-step2&quot;));
        job.waitForCompletion(true);
    }
}
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop2.x——倒排索引]]></title>
      <url>http://spark8.tech/2016/06/22/daopaisuoyin/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>文档中找单词，当文档数越来越多时，直接打开文档找单词不靠谱(一个一个文档找比较慢)。<br>倒排索引：将单词出现在文档中的位置，记录下来。<br>如：a  -&gt;  1.txt,11.txt,56.txt</p>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>1、计算某个单词出现在一个文档中的次数<br>MR1阶段</p>
<blockquote>
<p> 单词     文档名称 次数<br>  分词器 IK<br>  获取文件名 inputSplit<br>  我和我的祖国 一刻也不能分割<br>  我<br>  我的祖国<br>  祖国<br>  ……<br>Map ：<br>KEYOUT: 我:文档名称<br> VALUEOUT: 1<br>SHUFFLE:<br>我:文档名称  Iterable1,1,1,1,1,1,1,1<br>Reduce：<br>我:文档名称 8<br>KEYOUT: 我<br>VALUEOUT： 文档名称 8</p>
</blockquote>
<p>2、计算一个单词出现在所有文档中的次数<br>MR2阶段</p>
<blockquote>
<p>MAP<br>KEYIN:我<br>VALUEIN:文档名称 8<br>SHUFFLE:<br>我:  Iterable[“文档名称1 8”,”文档名称2 18”]<br>Reduce：<br>KEYOUT:我<br>VALUEOUT：权重值 ，总出现次数，“文档名称1 8”，“文档名称2 18”</p>
<h2 id="代码开发"><a href="#代码开发" class="headerlink" title="代码开发"></a>代码开发</h2><p>源码请参见<a href="www.baidu.com">github链接</a><br>1.驱动类</p>
</blockquote>
<pre><code>public class IndexJob {
public static void main(String[] args) throws Exception {
    String map1inputpath = &quot;&quot;;
    String reduce1outputmap2inputpath = &quot;&quot;;
    String respath = &quot;&quot;;

    //第一个job
    Job job = Job.getInstance();

    job.setMapperClass(IndexMap1.class);
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(IntWritable.class);

    job.setReducerClass(IndexReduce1.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);

    job.setNumReduceTasks(2);

    FileInputFormat.setInputPaths(job,new Path(&quot;/&quot;));
    FileOutputFormat.setOutputPath(job, new Path(&quot;/&quot;));

    if(job.waitForCompletion(true)){
        //第二个job
        //可不可以公用一个job
        job.setMapperClass(IndexMap2.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);

        job.setReducerClass(IndexReduce2.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        job.setNumReduceTasks(2);

        FileInputFormat.setInputPaths(job,new Path(&quot;/&quot;));
        FileOutputFormat.setOutputPath(job, new Path(&quot;/&quot;));

        job.waitForCompletion(true);
    }
}
}
</code></pre><p>2.IndexMap1类</p>
<pre><code>public class IndexMap1 extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; {

@Override
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
    byte[] bytes = value.getBytes();

    //2、对数据进行分词
    InputStream inputStream = new ByteArrayInputStream(bytes);
    Reader reader = new InputStreamReader(inputStream);
    IKSegmenter ikSegmenter = new IKSegmenter(reader, true);

    //获得文档名称
    FileSplit inputSplit = (FileSplit) context.getInputSplit();
    String fileName  = inputSplit.getPath().getName();

    //4、map端词条出现次数统计
    Lexeme t;
    while ((t = ikSegmenter.next()) != null) {
        context.write(new Text(t.getLexemeText()+&quot;:&quot;+fileName),new IntWritable(1));
    }
}
}
</code></pre><p>3.Index2Reduce1类</p>
<pre><code>public class IndexReduce1 extends Reducer&lt;Text,IntWritable,Text,Text&gt; {

@Override
protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
    Integer sum=0;
    for(IntWritable count:values){
        sum+=count.get();
    }
    String[] arrStr = key.toString().split(&quot;:&quot;);
    key.set(arrStr[0]);
    context.write(key,new Text(arrStr[1]+&quot;:&quot;+sum));
}
}
</code></pre><p>4.IndexMap2类</p>
<pre><code>public class IndexMap2 extends Mapper&lt;Text,LongWritable,Text,Text&gt; {

@Override
protected void map(Text key, LongWritable value, Context context) throws IOException, InterruptedException {
    String[] arrStr = value.toString().split(&quot; &quot;);
    context.write(new Text(arrStr[0]),new Text(arrStr[1]));
}
}
</code></pre><p>5.IndexReduce2类</p>
<pre><code>public class IndexReduce2 extends Reducer&lt;Text,Text,Text,Text&gt; {

@Override
protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {
    int sum =0;
    StringBuffer sb = new StringBuffer();
    for(Text txt:values){
        sb.append(txt);
        sb.append(&quot;,&quot;);
        String[] arrStr = values.toString().split(&quot;:&quot;);
        sum += Integer.parseInt(arrStr[1]);
    }
    context.write(key,new Text(sum + &quot;&quot; + &quot;\t&quot; + sb.toString()));
}
}
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop2.x——MR并行计算框架结构及核心运行机制]]></title>
      <url>http://spark8.tech/2016/06/22/mapreduce/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="一、MR结构"><a href="#一、MR结构" class="headerlink" title="一、MR结构"></a>一、MR结构</h2><p>一个完整的mapreduce程序在分布式运行时有三类实例进程：<br>1、MRAppMaster：负责整个程序的过程调度及状态协调<br>2、mapTask：负责map阶段的整个数据处理流程<br>3、ReduceTask：负责reduce阶段的整个数据处理流程</p>
<h2 id="二、MR程序运行的两种模式"><a href="#二、MR程序运行的两种模式" class="headerlink" title="二、MR程序运行的两种模式"></a>二、MR程序运行的两种模式</h2><p>本地运行模式：<br>1、mapreduce程序是被提交给LocalJobRunner在本地以单进程的形式运行而处理的数据及输出结果可以在本地文件系统，也可以在hdfs上<br>2、怎样实现本地运行？写一个程序，不要带集群的配置文件（本质是你的mr程序的conf中是否有mapreduce.framework.name=local以及yarn.resourcemanager.hostname参数）<br>3、本地模式非常便于进行业务逻辑的debug，<br>集群运行模式：<br>1、将mapreduce程序提交给yarn集群resourcemanager，分发到很多的节点上并发执行<br>2、处理的数据和输出结果应该位于hdfs文件系统<br>3、提交集群的实现步骤：<br>A、将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动<br>     $ hadoop jar wordcount.jar cn.itcast.bigdata.mrsimple.WordCountDriver inputpath outputpath<br>B、直接在linux的eclipse中运行main方法<br>（项目中要带参数：mapreduce.framework.name=yarn以及yarn的两个基本配置）<br>C、如果要在windows的eclipse中提交job给集群，则要修改YarnRunner类<br>参考链接<a href="http://www.flyne.org/article/1133" target="_blank" rel="external">http://www.flyne.org/article/1133</a></p>
<h2 id="三、MR在YARN上的运行全流程"><a href="#三、MR在YARN上的运行全流程" class="headerlink" title="三、MR在YARN上的运行全流程"></a>三、MR在YARN上的运行全流程</h2><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而mapreduce等运算程序则相当于运行于操作系统之上的应用程序。<br>1、    yarn并不清楚用户提交的程序的运行机制<br>2、    yarn只提供运算资源的调度（用户程序向yarn申请资源，yarn就负责分配资源）<br>3、    yarn中的主管角色叫ResourceManager<br>4、    yarn中具体提供运算资源的角色叫NodeManager<br>这样一来，yarn其实就与运行的用户程序完全解耦，就意味着yarn上可以运行各种类型的分布式运算程序（mapreduce只是其中的一种），比如mapreduce、storm程序，spark程序，Tez，只要各种程序实现AppMaster接口即可。<br>下图为Yarn实现MR程序在集群上的资源调度过程：<br><img src="/images/mrYarn.png" alt="" title="mrYarn"></p>
<h2 id="四、MR程序运行机制与shuffle"><a href="#四、MR程序运行机制与shuffle" class="headerlink" title="四、MR程序运行机制与shuffle"></a>四、MR程序运行机制与shuffle</h2><h3 id="MR运行流程"><a href="#MR运行流程" class="headerlink" title="MR运行流程"></a>MR运行流程</h3><p><img src="/images/mrlc.png" alt="" title="mr运行流程"><br>详细流程：<br>1、    maptask收集我们的map()方法输出的kv对，放到内存缓冲区中<br>2、    从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件<br>3、    多个溢出文件会被合并成大的溢出文件<br>4、    在溢出过程中，及合并的过程中，都要调用partitoner进行分组和针对key进行排序<br>5、    reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据<br>6、    reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序）<br>7、    合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法）</p>
<p>Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快<br>缓冲区的大小可以通过参数调整,  参数：io.sort.mb  默认100M</p>
<h3 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h3><p>mapreduce中，map阶段处理的数据如何传递给reduce阶段，是mapreduce框架中最关键的一个流程，这个流程就叫shuffle；<br>shuffle: 洗牌、发牌——（核心机制：数据分区，排序，缓存）；<br>具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区和排序。<br>shuffle是MR处理流程中的一个过程，它的每一个处理步骤是分散在各个map task和reduce task节点上完成的，整体来看，分为3个操作：<br>1.分区partition<br>2.Sort根据key排序<br>3.Combiner进行局部value的合并<br>在接下来的文章中将通过具体的实战去具体体现三个操作的作用<br><a href="www.baidu.com">链接1</a> <a href="%E9%93%BE%E6%8E%A52">链接2</a> <a href="www.baidu.com">链接3</a> </p>
<h2 id="五、MapTask并行度决定机制与切片"><a href="#五、MapTask并行度决定机制与切片" class="headerlink" title="五、MapTask并行度决定机制与切片"></a>五、MapTask并行度决定机制与切片</h2><h3 id="MapTask并行度决定机制"><a href="#MapTask并行度决定机制" class="headerlink" title="MapTask并行度决定机制"></a>MapTask并行度决定机制</h3><p>一个job的map阶段并行度由客户端在提交job时决定<br>而客户端对map阶段并行度的规划的基本逻辑为：<br>将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多个split），然后每一个split分配一个mapTask并行实例处理<br>这段逻辑及形成的切片规划描述文件，由FileInputFormat实现类的getSplits()方法完成。</p>
<h3 id="FileInputFormat切片机制"><a href="#FileInputFormat切片机制" class="headerlink" title="FileInputFormat切片机制"></a>FileInputFormat切片机制</h3><p>a)    简单地按照文件的内容长度进行切片<br>b)    切片大小，默认等于block大小<br>c)    切片时不考虑数据集整体，而是逐个针对每一个文件单独切片<br>默认情况下，切片大小=blocksize，但是，不论怎么调参数，都不能让多个小文件“划入”一个split。<br>选择并发数的影响因素：<br>运算节点的硬件配置<br>运算任务的类型：CPU密集型还是IO密集型<br>运算任务的数据量<br><img src="/images/mrsplit.png" alt="" title="mrsplit"></p>
<h3 id="map并行度的经验"><a href="#map并行度的经验" class="headerlink" title="map并行度的经验"></a>map并行度的经验</h3><p>如果硬件配置为2*12core + 64G，恰当的map并行度是大约每个节点20-100个map，最好每个maptask的执行时间至少一分钟。<br>如果job的每个map或者 reduce task的运行时间都只有30-40秒钟，那么就减少该job的map或者reduce数，每一个task(map|reduce)的setup和加入到调度器中进行调度，这个中间的过程可能都要花费几秒钟，所以如果每个task都非常快就跑完了，就会在task的开始和结束的时候浪费太多的时间。<br>配置task的JVM重用可以改善该问题：<br>（mapred.job.reuse.jvm.num.tasks，默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM）</p>
<p>如果input的文件非常的大，比如一个文件2G，同时，机器性能又比较好，那么，可以考虑将hdfs上的每个block size设大，比如设成256MB或者512MB<br>\&lt;所谓经验，是在特定数据、特定机器配置、特定业务逻辑场景下，试出来的></p>
<h2 id="六、ReduceTask并行度的决定"><a href="#六、ReduceTask并行度的决定" class="headerlink" title="六、ReduceTask并行度的决定"></a>六、ReduceTask并行度的决定</h2><p>reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置：</p>
<p>//默认值是1，手动设置为4<br>job.setNumReduceTasks(4);</p>
<p>如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜<br>注意： reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask</p>
<p>尽量不要运行太多的reduce task。对大多数job来说，最好rduce的个数最多和集群中的reduce持平，或者比集群的 reduce slots小。这个对于小集群而言，尤其重要。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop2.x——分布式缓存(解决数据倾斜问题)]]></title>
      <url>http://spark8.tech/2016/06/22/fbshchadoop/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>上篇关于左连接的文章存在如下的问题<br>假设公司商品的数量有限，商品表记录不是有限<br>订单表：500万订单  产生一千万记录<br>商品表：100个产品   产生100条记录<br>仅要求在订单表后面加入商品名称这个需求，如果maptask分别从hdfs上远程读取订单表和商品表，再运行多个reducetask，网络和资源开销变得非常大，如果有将商品表缓存到运行maptask的本地nodemanager中，那么就可以省略reduce过程，极大的减少网络和资源的开销。</p>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>1.Job通过addCacheFile方法将商品表加入到缓存文件<br>2.在Map类中重写setup方法，从context中获取getCacheFile(“商品表”)，将商品信息加入到成员变量中<br>3 map方法中将商品信息加入到订单表</p>
<h2 id="代码开发"><a href="#代码开发" class="headerlink" title="代码开发"></a>代码开发</h2><p>源码参见链接<a href="www.baidu.com" title="github">github</a><br>1.驱动类<br><img src="/images/fbshcqdl.png" alt="" title="fbshcqd"><br>2.Map类<br><img src="/images/fbshcmap.png" alt="" title="fbshcmap"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop2.x——自定义GroupingComparator分组取Top1]]></title>
      <url>http://spark8.tech/2016/06/22/groupingCompa/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h1><p>有如下订单数据<br><img src="/images/ddsj.png" alt="" title="ddsj"><br>现在需要求出每一个订单中成交金额最大的一笔交易</p>
<h2 id="实现分析"><a href="#实现分析" class="headerlink" title="实现分析"></a>实现分析</h2><p>1.利用“订单id和成交金额”作为key，可以将map阶段读取到的所有订单数据按照id分区，按照金额排序，发送到reduce<br>2、在reduce端利用groupingcomparator将订单id相同的kv聚合成组，然后取第一个即是最大值</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>自定义groupingcomparator</p>
<pre><code>/**
*用于控制shuffle过程中reduce端对kv对的聚合逻辑
*
*/
   public class ItemidGroupingComparator extends WritableComparator {

       protected ItemidGroupingComparator() {

       super(OrderBean.class, true);
       }


       @Override
       public int compare(WritableComparable a, WritableComparable b) {
       OrderBean abean = (OrderBean) a;
       OrderBean bbean = (OrderBean) b;

       //将item_id相同的bean都视为相同，从而聚合为一组
       return abean.getItemid().compareTo(bbean.getItemid());
       }
   }
</code></pre><p>定义订单信息bean</p>
<pre><code>/**
 *订单信息bean，实现hadoop的序列化机制
 *
 */
public class OrderBean implements WritableComparable&lt;OrderBean&gt;{
    private Text itemid;
    private DoubleWritable amount;

    public OrderBean() {
    }
    public OrderBean(Text itemid, DoubleWritable amount) {
        set(itemid, amount);
    }

    public void set(Text itemid, DoubleWritable amount) {

        this.itemid = itemid;
        this.amount = amount;

    }

    public Text getItemid() {
        return itemid;
    }

    public DoubleWritable getAmount() {
        return amount;
    }

    @Override
    public int compareTo(OrderBean o) {
        int cmp = this.itemid.compareTo(o.getItemid());
        if (cmp == 0) {

        cmp = -this.amount.compareTo(o.getAmount());
        }
        return cmp;
    }

    @Override
    public void write(DataOutput out) throws IOException {
    out.writeUTF(itemid.toString());
    out.writeDouble(amount.get());

    }

    @Override
    public void readFields(DataInput in) throws IOException {
        String readUTF = in.readUTF();
        double readDouble = in.readDouble();

        this.itemid = new Text(readUTF);
        this.amount= new DoubleWritable(readDouble);
    }


    @Override
    public String toString() {
        return itemid.toString() + &quot;\t&quot; + amount.get();
    }
}
</code></pre><p>编写mapreduce处理流程</p>
<pre><code>/**
利用secondarysort机制输出每种item订单金额最大的记录
@author duanhaitao@itcast.cn
 *
 */
public class SecondarySort {

static class SecondarySortMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt;{

    OrderBean bean = new OrderBean();

@Override
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

    String line = value.toString();
    String[] fields = StringUtils.split(line, &quot;\t&quot;);

    bean.set(new Text(fields[0]), new DoubleWritable(Double.parseDouble(fields[1])));

    context.write(bean, NullWritable.get());

    }

}

static class SecondarySortReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt;{


//在设置了groupingcomparator以后，这里收到的kv数据 就是：  &lt;1001 87.6&gt;,null  &lt;1001 76.5&gt;,null  .... 
//此时，reduce方法中的参数key就是上述kv组中的第一个kv的key：&lt;1001 87.6&gt;
//要输出同一个item的所有订单中最大金额的那一个，就只要输出这个key
@Override
protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {
    context.write(key, NullWritable.get());
    }
}


public static void main(String[] args) throws Exception {

    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);

    job.setJarByClass(SecondarySort.class);

    job.setMapperClass(SecondarySortMapper.class);
    job.setReducerClass(SecondarySortReducer.class);


    job.setOutputKeyClass(OrderBean.class);
    job.setOutputValueClass(NullWritable.class);

    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    //指定shuffle所使用的GroupingComparator类
    job.setGroupingComparatorClass(ItemidGroupingComparator.class);
    //指定shuffle所使用的partitioner类
    job.setPartitionerClass(ItemIdPartitioner.class);

    job.setNumReduceTasks(3);

    job.waitForCompletion(true);

}

}
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop2.x——MR参数优化]]></title>
      <url>http://spark8.tech/2016/06/22/mrziyuanyouhua/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="1-资源相关参数"><a href="#1-资源相关参数" class="headerlink" title="1.  资源相关参数"></a>1.  资源相关参数</h2><p>以下参数是在用户自己的mr应用程序中配置就可以生效</p>
<blockquote>
<p>(1) mapreduce.map.memory.mb: 一个Map Task可使用的资源上限（单位:MB），默认为1024。如果Map Task实际使用的资源量超过该值，则会被强制杀死。<br>(2) mapreduce.reduce.memory.mb: 一个Reduce Task可使用的资源上限（单位:MB），默认为1024。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。<br>(3) mapreduce.map.java.opts: Map Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc” （@taskid@会被Hadoop框架自动换为相应的taskid）, 默认值: “”<br>(4) mapreduce.reduce.java.opts: Reduce Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc”, 默认值: “”<br>(5) mapreduce.map.cpu.vcores: 每个Map task可使用的最多cpu core数目, 默认值: 1<br>(6) mapreduce.reduce.cpu.vcores: 每个Reduce task可使用的最多cpu core数目, 默认值: 1</p>
</blockquote>
<p>//应该在yarn启动之前就配置在服务器的配置文件中才能生效</p>
<blockquote>
<p>(7) yarn.scheduler.minimum-allocation-mb      1024   给应用程序container分配的最小内存<br>(8) yarn.scheduler.maximum-allocation-mb      8192    给应用程序container分配的最大内存<br>(9) yarn.scheduler.minimum-allocation-vcores    1<br>(10)yarn.scheduler.maximum-allocation-vcores    32<br>(11)yarn.nodemanager.resource.memory-mb   8192  </p>
</blockquote>
<p>//shuffle性能优化的关键参数，应在yarn启动之前就配置好</p>
<blockquote>
<p>(12)mapreduce.task.io.sort.mb   100         //shuffle的环形缓冲区大小，默认100m<br>(13)mapreduce.map.sort.spill.percent   0.8    //环形缓冲区溢出的阈值，默认80%</p>
<h2 id="2-容错相关参数"><a href="#2-容错相关参数" class="headerlink" title="2. 容错相关参数"></a>2. 容错相关参数</h2><p>(1) mapreduce.map.maxattempts: 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。<br>(2) mapreduce.reduce.maxattempts: 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。<br>(3) mapreduce.map.failures.maxpercent: 当失败的Map Task失败比例超过该值为，整个作业则失败，默认值为0. 如果你的应用程序允许丢弃部分输入数据，则该该值设为一个大于0的值，比如5，表示如果有低于5%的Map Task失败（如果一个Map Task重试次数超过mapreduce.map.maxattempts，则认为这个Map Task失败，其对应的输入数据将不会产生任何结果），整个作业扔认为成功。<br>(4) mapreduce.reduce.failures.maxpercent: 当失败的Reduce Task失败比例超过该值为，整个作业则失败，默认值为0.<br>(5) mapreduce.task.timeout: Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该task处于block状态，可能是卡住了，也许永远会卡主，为了防止因为用户程序永远block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是300000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示  是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。</p>
<h2 id="3-本地运行mapreduce-作业"><a href="#3-本地运行mapreduce-作业" class="headerlink" title="3.  本地运行mapreduce 作业"></a>3.  本地运行mapreduce 作业</h2><p>设置以下几个参数:<br>mapreduce.framework.name=local<br>mapreduce.jobtracker.address=local<br>fs.defaultFS=local</p>
<h2 id="4-效率和稳定性相关参数"><a href="#4-效率和稳定性相关参数" class="headerlink" title="4. 效率和稳定性相关参数"></a>4. 效率和稳定性相关参数</h2><p>(1) mapreduce.map.speculative: 是否为Map Task打开推测执行机制，默认为false<br>(2) mapreduce.reduce.speculative: 是否为Reduce Task打开推测执行机制，默认为false<br>(3) mapreduce.job.user.classpath.first &amp; mapreduce.task.classpath.user.precedence：当同一个class同时出现在用户jar包和hadoop jar中时，优先使用哪个jar包中的class，默认为false，表示优先使用hadoop jar中的class。<br>(4) mapreduce.input.fileinputformat.split.minsize: FileInputFormat做切片时的最小切片大小，(5)mapreduce.input.fileinputformat.split.maxsize:  FileInputFormat做切片时的最大切片大小 (切片的默认大小就等于blocksize，即 134217728) </p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark1.6—RDD自定义分区器Partitioner]]></title>
      <url>http://spark8.tech/2016/06/21/sparkfqq/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h1><p>统计网站子域名下点击url页面排名前三的点击次数</p>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>用之前scala中的思路是先groupBy再mapValues,在单机中没有问题但是在分布式系统中,groupBy意味着大量的网络通信,要极力避免。<br>默认的hashPartition会造成shuffle后的结果有的分区落的数据少、有的数据大，数据有倾斜，大量数据在某台机器或者某几台机器计算，容易拉长计算时间和内存溢出，所以可以考虑自定生成key，比如四个分区就将key设为(0,1,2,3)，但相对比较麻烦，自定义分区器，可以避免hash碰撞。<br>所以此处采用自定义分区器实现，有多少个子域名就分为多少个区，再通过mapPartition使得排序再本地进行，这样明显的会提高性能。<br>下面将分别列出两种方式代码</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>groupBy方式</p>
<pre><code>object UrlCount3 {

def main(args: Array[String]) {
val conf = new SparkConf().setAppName(&quot;UrlCount3&quot;).setMaster(&quot;local&quot;)
val sc = new SparkContext(conf)
//read data
val rdd1 = sc.textFile(&quot;/Users/Variant/Desktop/itcast.log&quot;).map(x =&gt; {
  val fields = x.split(&quot;\t&quot;)
  val time = fields(0)
  val url = fields(1)
  (url, 1)
})

val urlCounts: RDD[(String, Int)] = rdd1.reduceByKey(_ + _)

//将数据缓存到内存(cache是一个 transformattion)
//放到分布式系统中去考虑
val cachedHosts: RDD[(String, (String, Int))] = urlCounts.map(x =&gt; {
  val url = x._1
  val host = new URL(url).getHost
  val count = x._2
  (host, (url, count))
}).cache()

/**
  * 以下为第一种方式,也是常规的方式
  * 先groupBy再mapValues,在单机中而言没有问题
  * 但是在分布式系统中,groupby就意味着大量的网络通信,要极力避免
  */
val rdd3 = urlCounts.map(x =&gt; {
  val url = x._1
  val count = x._2
  val host = new java.net.URL(url).getHost
  (host, url, count)
})

val result = rdd3.groupBy(_._1).mapValues(_.toList.sortBy(_._3).reverse.take(3))
result.saveAsTextFile(&quot;/Users/Variant/Desktop/result&quot;)
}
}
</code></pre><p>分区器</p>
<pre><code>object UrlCount3 {

  def main(args: Array[String]) {
val conf = new SparkConf().setAppName(&quot;UrlCount3&quot;).setMaster(&quot;local&quot;)
val sc = new SparkContext(conf)
//read data
val rdd1 = sc.textFile(&quot;/Users/Variant/Desktop/itcast.log&quot;).map(x =&gt; {
  val fields = x.split(&quot;\t&quot;)
  val time = fields(0)
  val url = fields(1)
  (url, 1)
})

val urlCounts: RDD[(String, Int)] = rdd1.reduceByKey(_ + _)

//将数据缓存到内存(cache是一个 transformattion)
//放到分布式系统中去考虑
val cachedHosts: RDD[(String, (String, Int))] = urlCounts.map(x =&gt; {
  val url = x._1
  val host = new URL(url).getHost
  val count = x._2
  (host, (url, count))
}).cache()

// action才能返回一个值,此处要用collect
val disctHosts = cachedHosts.keys.distinct().collect()
/**
  * 第二种方式:自定义分区器(只能应用于K-V数据)
  * 有的分区落的数据少、有的数据大，就会造成数据倾斜，大量数据跑到某台机器或者某几台机器，容易跑的很慢和内存溢出
  * 默认是hashPartition，所以可以考虑自定生成key，比如四个分区就将key设为(0,1,2,3)
  * 第二是自定义分区器，可以避免hash碰撞，mapPartition在本地排序
  */
val hostPartitioner = new HostPartitioner(disctHosts)
val partitioned: RDD[(String, (String, Int))] = cachedHosts.partitionBy(hostPartitioner)

val finalResult = partitioned.mapPartitions(it =&gt; {
  it.toList.sortBy(_._2._2).reverse.take(3).iterator
})

finalResult.saveAsTextFile(&quot;/Users/Variant/Desktop/result&quot;)
sc.stop()
}
}
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark1.6—RDD广播变量]]></title>
      <url>http://spark8.tech/2016/06/21/sparkgb/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h1><p>根据IP库，统计网站的IP来源，哪些区域比较集中，为广告投放市场做决策依据<br><img src="/images/iptxt.png" alt="ip.txt"><br><img src="/images/httpdata.png" alt="httpdata.log"></p>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>ip库数据量比较小，可将数据完整的广播到同属一个Application的Executor上，使RDD各分区在本地计算<br>1.new SparkContext<br>2.从ip库里面读取需要的字段startNum,endNum,province<br>3.collect先将信息返回至客户端,将ip库提取的信息广播到所有的Worker-Excutor<br>4.将请求信息映射为省份,通过二分查找求出请求信息在ip库的下标,返回(province,1)RDD<br>5.reduceByKey,并将数据foreachPartition存库</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><pre><code>object IPLocation {

  val data2MySQL = (iterator: Iterator[(String, Int)]) =&gt; {
    var conn: Connection = null
    var ps : PreparedStatement = null
    val sql = &quot;INSERT INTO location_info (location, counts, accesse_date) VALUES (?, ?, ?)&quot;
    try {
      conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3307/test?useUnicode=true&amp;characterEncoding=UTF-8&quot;, &quot;root&quot;, &quot;hldw3101&quot;)
      iterator.foreach(line =&gt; {
        ps = conn.prepareStatement(sql)
        ps.setString(1, line._1)
        ps.setInt(2, line._2)
        ps.setDate(3, new Date(System.currentTimeMillis()))
        ps.executeUpdate()
      })
    } catch {
      case e: Exception =&gt; println(e.printStackTrace())
    } finally {
      if (ps != null)
        ps.close()
      if (conn != null)
        conn.close()
    }
  }

  def ip2Long(ip: String): Long = {
    val fragments = ip.split(&quot;[.]&quot;)
    var ipNum = 0L
    for (i &lt;- 0 until fragments.length){
      ipNum =  fragments(i).toLong | ipNum &lt;&lt; 8L
    }
    ipNum
  }

  def binarySearch(lines: Array[(String, String, String)], ip: Long) : Int = {
    var low = 0
    var high = lines.length - 1
    while (low &lt;= high) {
      val middle = (low + high) / 2
      if ((ip &gt;= lines(middle)._1.toLong) &amp;&amp; (ip &lt;= lines(middle)._2.toLong))
        return middle
      if (ip &lt; lines(middle)._1.toLong)
        high = middle - 1
      else {
        low = middle + 1
      }
    }
    -1
  }

  /**
    * 数据要频繁的使用,但不会怎么经常变
    * 广播变量的应用场景
    * 1.new SparkContext
    * 2.从ip库里面读取需要的字段startNum,endNum,province
    * 3.collect先将信息返回至客户端,将ip库提取的信息广播到所有的Worker-Excutor
    * 4.将请求信息映射为省份,通过二分查找求出请求信息在ip库的下标,返回(province,1)RDD
    * 5.reduceByKey,并将数据foreachPartition存库
    * @param args
    */
  def main(args: Array[String]) {

    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;IpLocation&quot;)
    val sc = new SparkContext(conf)

    val ipConfs = sc.textFile(&quot;/Users/Variant/Desktop/ip.txt&quot;).map(line =&gt; {
      val fields = line.split(&quot;\\|&quot;)
      val startNum = fields(2)
      val endNum = fields(3)
      val province = fields(6)
      (startNum, endNum, province)
    }).collect()

    //将数据广播到所有属于这个App的Executor上
    //ipConfs分布在Worker Executor所在的机器上
    //将ipConfs的数据返回到Driver,Driver再讲完整的数据广播到同属一个Application的Executor上
    val broadcast: Broadcast[Array[(String, String, String)]] = sc.broadcast(ipConfs)

    val provinceAndOne = sc.textFile(&quot;/Users/Variant/Desktop/httpdata.log&quot;).map(x =&gt; {
      val fields = x.split(&quot;\\|&quot;)
      val ip = fields(1)
      val ipLong = ip2Long(ip)
      val arr = broadcast.value
      val index = binarySearch(arr, ipLong)
      val province = arr(index)._3
      (province, 1)
    })

    val provinceCount = provinceAndOne.reduceByKey(_+_)
    //println(provinceCount.collect().toBuffer)
    provinceCount.foreachPartition(data2MySQL)

    sc.stop()

  }
}
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark1.6—自定义排序规则，对比Hadoop MR]]></title>
      <url>http://spark8.tech/2016/06/21/sparksortby/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>使用隐式转换，自定义排序规则</p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>第一种方式：case class Girl 混入 Ordered<a href="#">Girl</a>接口，实现comapre方法，但是明显的高耦合<br>第二种方式：使用隐式转换，按sortBy要求 传入隐式值girlOrdering，实现解耦和自定义排序<br><img src="/images/sortby.png" alt=""></p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>第一种方式：</p>
<pre><code>object CustomSort {

  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName(&quot;CustomSort&quot;).setMaster(&quot;local[2]&quot;)
    val sc = new SparkContext(conf)
    val rdd1 = sc.parallelize(List((&quot;yuihatano&quot;, 90, 28, 1), (&quot;angelababy&quot;, 90, 27, 2),(&quot;JuJingYi&quot;, 95, 22, 3)))
    import OrderContext._
    val rdd2 = rdd1.sortBy(x =&gt; Girl(x._2, x._3), false)
    println(rdd2.collect().toBuffer)
    sc.stop()
  }

}

/**
  * 第一种方式
  * @param faceValue
  * @param age
*/
case class Girl(val faceValue: Int, val age: Int) extends Ordered[Girl] with Serializable {
  override def compare(that: Girl): Int = {
    if(this.faceValue == that.faceValue) {
      that.age - this.age
    } else {
      this.faceValue -that.faceValue
    }
  }
}
</code></pre><p>第二种方式：</p>
<pre><code>case class Girl(faceValue: Int, age: Int) extends Serializable

object OrderContext {
  implicit val girlOrdering  = new Ordering[Girl] {
    override def compare(x: Girl, y: Girl): Int = {
      if(x.faceValue &gt; y.faceValue) 1
      else if (x.faceValue == y.faceValue) {
        if(x.age &gt; y.age) -1 else 1
      } else -1
    }
  }
}

object CustomSort {

  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName(&quot;CustomSort&quot;).setMaster(&quot;local[2]&quot;)
    val sc = new SparkContext(conf)
    val rdd1 = sc.parallelize(List((&quot;yuihatano&quot;, 90, 28, 1), (&quot;angelababy&quot;, 90, 27, 2),(&quot;JuJingYi&quot;, 95, 22, 3)))
    import OrderContext._
    val rdd2 = rdd1.sortBy(x =&gt; Girl(x._2, x._3), false)
    println(rdd2.collect().toBuffer)
    sc.stop()
  }

}
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark1.6— RDD 根据电信基站数据计算用户常在地点]]></title>
      <url>http://spark8.tech/2016/06/20/rdddx/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h1><p>现有电信运营商脱敏处理后提供的基站收集到的用户数据，包括电话号码、时间戳、基站信息、信号类型(断开/连接)，以及基站数据对应经纬度信息，求用户前三常在地点。<br><img src="/images/jzsj2.png" alt=""><br><img src="/images/wzsj2.png" alt=""></p>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>1.求用户常在地点，即求用户所停留时间最多的基站，即求信号在0和1之间时间最长的基站<br>2.根据信号类型，将连接时时间戳设为负数，以（手机号+基站）为key，时间为value求和即求出用户在各基站的时间<br>3.1 按地理位置join两个表的数据，再按手机号码分组，取前二<br>3.2 先按手机号码分组，取前二，再join两个表<br>第一种方案存在问题是表数据太大，计算慢<br>第二种方式明显要好很多，但是排序后返回的RDD类型是List，不是元组，所以先转变成元组再join</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>第一种方案代码如下：</p>
<pre><code>object MoblieLocation {

  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName(&quot;MoblieLocation&quot;).setMaster(&quot;local[2]&quot;)
    val sc = new SparkContext(conf)

    val fnAndLacTime: RDD[((String, String), Long)]= sc.textFile(&quot;/Users/Variant/Desktop/bs_log&quot;).map(line =&gt; {
      //切分字段
      val fields = line.split(&quot;,&quot;)
      //手机号
      val fn = fields(0)
      //时间
      val time = fields(1).toLong
      //基站ID
      val lac = fields(2)
      // 事件类型
      val eventType = fields(3)
      val time_long = if(eventType == &quot;1&quot;) -time else time
      //((手机，基站ID),时间）
      ((fn, lac), time_long)
    })

    val sumedFnAndLacTime: RDD[((String, String), Long)] = fnAndLacTime.reduceByKey(_+_)

    val fnAndLacAndTime = sumedFnAndLacTime.map(x =&gt; {
      val fn = x._1._1
      val lac = x._1._2
      val timeSum = x._2
      //(fn, lac, timeSum)
      (lac, (fn, timeSum))
    })

    //根据手机号进行分组
 //   val grouped: RDD[(String, Iterable[(String, String, Long)])] = fnAndLacAndTime.groupBy(_._1)
    //在迭代器中排序取前两个
 //   val sorted: RDD[List[(String, String, Long)]] = grouped.mapValues(_.toList.sortBy(_._3).reverse.take(2)).values

    val lacInfo = sc.textFile(&quot;/Users/Variant/Desktop/lac_info.txt&quot;).map(line =&gt; {
      val fields = line.split(&quot;,&quot;)
      val lacId = fields(0)
      val x = fields(1)
      val y = fields(2)
      (lacId, (x, y))
    })

  val joinInfo: RDD[(String, ((String, Long), (String, String)))] = fnAndLacAndTime.join(lacInfo)

  val finalResult:RDD[(String,String,Long,(String,String))] = joinInfo.map(x =&gt; {
      val fn = x._2._1._1
      val lac = x._1
      val time = x._2._1._2
      val  xy = x._2._2
      (fn, lac, time, xy)
    })
    val finalValues: RDD[(String, List[(String, String, Long, (String, String))])] = finalResult.groupBy(_._1).mapValues(_.toList.sortBy(_._3).reverse.take(2))

    println(finalValues.collect().toBuffer)

    //val r = fnAndLacTime.collect().toBuffer

    sc.stop()
  }

}
</code></pre><p>第二种方案代码如下：</p>
<pre><code>object MoblieLocation {

  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName(&quot;MoblieLocation&quot;).setMaster(&quot;local[2]&quot;)
    val sc = new SparkContext(conf)

    val fnAndLacTime: RDD[((String, String), Long)]= sc.textFile(&quot;/Users/Variant/Desktop/bs_log&quot;).map(line =&gt; {
      //切分字段
      val fields = line.split(&quot;,&quot;)
      //手机号
      val fn = fields(0)
      //时间
      val time = fields(1).toLong
      //基站ID
      val lac = fields(2)
      // 事件类型
      val eventType = fields(3)
      val time_long = if(eventType == &quot;1&quot;) -time else time
      //((手机，基站ID),时间）
      ((fn, lac), time_long)
    })

    val sumedFnAndLacTime: RDD[((String, String), Long)] = fnAndLacTime.reduceByKey(_+_)

    val fnAndLacAndTime = sumedFnAndLacTime.map(x =&gt; {
      val fn = x._1._1
      val lac = x._1._2
      val timeSum = x._2
      (fn, lac, timeSum)
      //(lac, (fn, timeSum))
    })

    //根据手机号进行分组
    val grouped: RDD[(String, Iterable[(String, String, Long)])] = fnAndLacAndTime.groupBy(_._1)
    //在迭代器中排序取前两个
    val sorted: RDD[List[(String, String, Long)]] = grouped.mapValues(_.toList.sortBy(_._3).reverse.take(2)).values

    val tupled: RDD[(String, (String, Long))] = sorted.flatMap(x =&gt; x).map(x =&gt; (x._2, (x._1, x._3)))

    val lacInfo = sc.textFile(&quot;/Users/Variant/Desktop/lac_info.txt&quot;).map(line =&gt; {
      val fields = line.split(&quot;,&quot;)
      val lacId = fields(0)
      val x = fields(1)
      val y = fields(2)
      (lacId, (x, y))
    })

    val joinInfo: RDD[(String, ((String, Long), (String, String)))] = tupled.join(lacInfo)


  val finalResult:RDD[(String,String,Long,(String,String))] = joinInfo.map(x =&gt; {
      val fn = x._2._1._1
      val lac = x._1
      val time = x._2._1._2
      val  xy = x._2._2
      (fn, lac, time, xy)
    })
    val finalValues: RDD[(String, List[(String, String, Long, (String, String))])] = finalResult.groupBy(_._1).mapValues(_.toList.sortBy(_._3).reverse.take(2))

    println(finalValues.saveAsTextFile(&quot;/Users/Variant/Desktop/result&quot;))

    sc.stop()
  }

}
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Scala——使用Currry实现视图界定]]></title>
      <url>http://spark8.tech/2016/06/18/scalacurry/</url>
      <content type="html"><![CDATA[<p>\&lt;!–more–><br>上文中使用了视图界定来解决类与类型的耦合以及规则的耦合，那么不使用视图界定可不可以呢？当然是可以的，这次就用柯里化（Curry）+隐式转换实现视图界定和上下文界定同样的效果。<br>代码如下：</p>
<pre><code>class Girl(val name:String,var age :Int, val faceValue : Int = 99){

}
object Spark8Predef {
//T =&gt; Ordered[T]
implicit def girlToPredef(girl : Girl) = new Ordered[Girl]{
  override def compare(that: Girl): Int = {
    if(girl.faceValue == that.faceValue) {
      that.age - girl.age
    } else {
      girl.faceValue - that.faceValue
    }
  }
}
//T =&gt;ordering[T] 
implicit object orderingGril extends Ordering[Girl] {
  override def compare(x: Girl, y: Girl): Int = {

    if (x.faceValue == y.faceValue) {
      y.age - x.age
    } else {
      x.faceValue - y.faceValue
    }
  }
}

}

class ChooseMachine[T] {

  def chooseGirl(a: T, b: T)(implicit ord: T =&gt; Ordered[T]): T = {
    // T类型要有&gt;方法,就要拥有T =&gt; Ordered[T] 的函数或者方法
    if (a &gt; b) a else b
  }

  def choose2(first: T, second: T)(implicit ord : Ordering[T]) : T = {
    import Ordered.orderingToOrdered
    if(first &gt; second) first else second
  }
}

/**
  * 想要实现与viewbound一样的功能，传入一个隐式转换函数
   */
object ChooseMachine {
  def main(args: Array[String]) {
    import Spark8Predef._
    val chooser = new ChooseMachine[Girl]
    val g1 = new Girl(&quot;tingting&quot;, 18, 85)
    val g2 = new Girl(&quot;angelaBaby&quot;, 20, 95)
    val choose = chooser.chooseGirl(g1, g2)

    println(choose.name)
  }

}
</code></pre><p>其实整个代码还是蛮简单的，就是用ChooseMachine的泛型，泛型参数不是视图界定，那么就在ChooseMachine的具体方法上实现柯里化，然后在导入Spark8Predef里面的隐式转换方法，就OK啦。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Scala—隐式转换实践]]></title>
      <url>http://spark8.tech/2016/06/18/scalayszh/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<p>业务以最简单的按字段compare为例。文章分为二个部分，第一种方式是没有用隐式转换的代码效果，第二种是用视图界定+隐式转换的代码效果，第三种是用上下文界定+隐式转换的代码效果</p>
<h2 id="不用隐式转换"><a href="#不用隐式转换" class="headerlink" title="不用隐式转换"></a>不用隐式转换</h2><p>代码如下：     </p>
<pre><code>class Girl(val name:String,var age :Int, val faceValue : Int = 99) extends Comparable[Girl]{
  override def compareTo(o: Girl): Girl = {
    ithis.faceValue - o.faceValue   
  }
}
class ChooseMachine[T &lt;:Comparable[T]] {

  def chooseGirl(a :T,b:T): T ={
    if(a.compareTo(b) &gt; 0) a else b
  }
}
object Girl{
  def main(args: Array[String]) {
val chooser = new ChooseMachine[Girl]
val g1 = new Girl(&quot;tingting&quot;,18,85)
val g2 = new Girl(&quot;angelaBaby&quot;,20,95)
val choose = chooser.chooseGirl(g1,g2)
println(choose.name)
}
</code></pre><p>第一种方式<br>1.这是最原始的方式,ChooseMichine对Girl进行排序,Girl需要继承并实现Comparable接口,并实现compareTo方法。ChooseMichine再在chooseGirl调用compareTo实现比较而得出结果<br>2.那么这样就存在两个问题<br> 一是排序的方式与类耦合在一起,如果我有Boy类需要按同样的方式比较,Boy类又要实现Comparable接口并实现compareTo方法<br> 二是同一类型多种排序方式无法共存?</p>
<h2 id="用视图界定-隐式转换"><a href="#用视图界定-隐式转换" class="headerlink" title="用视图界定+隐式转换"></a>用视图界定+隐式转换</h2><p>代码如下：</p>
<pre><code>class Girl(val name:String,var age :Int, val faceValue : Int = 99){

}
//T &lt;% Ordered[T] 视图界定,可以通过隐式转换实现
class ChooseMachine[T &lt;% Ordered[T]] {
  def chooseGirl(a :T,b:T): T ={
    // T类型要有&gt;方法,就要拥有T =&gt; Ordered[T] 的函数或者方法
    if(a &gt; b) a else b
  }
}
object Spark8Predef {

  implicit val girlToPredef = (girl : Girl)  =&gt; new Ordered[Girl]{
      override def compare(that: Girl): Int = {
        if (girl.faceValue == that.faceValue) {
          that.age - girl.age
        } else {
          girl.faceValue - that.faceValue
        }
      }
    }
/*//用方法就这样写
implicit def girlToPredef(girl : Girl) = new Ordered[Girl]{
  override def compare(that: Girl): Int = {
    if(girl.faceValue == that.faceValue) {
      that.age - girl.age
    } else {
      girl.faceValue - that.faceValue
    }
  }
}*/    
}
def main(args: Array[String]) {
  import Spark8Predef._
  val chooser = new ChooseMachine[Girl]
  val g1 = new Girl(&quot;tingting&quot;, 18, 85)
  val g2 = new Girl(&quot;angelaBaby&quot;, 20, 95)
  val choose = chooser.chooseGirl(g1, g2)

  println(choose.name)
}
</code></pre><p>第二种方式<br> 1.解决排序与类耦合,使用视图界定,将T类型转换为Ordered[T]类型,<br>  这样就可以实现用不同的类进行比较，new MissRight[Girl],可以换成new MissRight[Boy]<br> 2.二是各种类的比较规则不会和类耦合在一起，只需要在不同程序导入不同的隐式转换就可以实现程序的灵活性。girlToOrdered  也可以写成 boyToOrdered<br> 3.使用T &lt;% Ordered[T] 需要隐式地传入一个T => Ordered<a href="#">T</a>的方法</p>
<h2 id="用上下文界定-隐式转换"><a href="#用上下文界定-隐式转换" class="headerlink" title="用上下文界定+隐式转换"></a>用上下文界定+隐式转换</h2><p>代码如下</p>
<pre><code>class Girl(val name:String,var age :Int, val faceValue : Int = 99){

}
//一个冒号  是Context bound  他需要一个隐式转换的值
class ChooseMachine [T : Ordering] {
  def select(first: T, scecond: T): T = {
    val ord = implicitly[Ordering[T]]
    if(ord.gt(first, scecond)) first else scecond
  }
}
object Spark8Predef {

/**
  *用Ordering[T] 代替Ordered,此处隐式的传入一个 Ordering[T]对象
  */
implicit object orderingGril extends Ordering[Girl] {
  override def compare(x: Girl, y: Girl): Int = {

    if (x.faceValue == y.faceValue) {
      y.age - x.age
    } else {
      x.faceValue - y.faceValue
    }
  }
}
}

object ChooseMachine {
  def main(args: Array[String]) {
    import Spark8Predef._
    val chooser = new ChooseMachine[Girl]
    val g1 = new Girl(&quot;tingting&quot;, 18, 85)
    val g2 = new Girl(&quot;angelaBaby&quot;, 20, 95)
    val choose = chooser.chooseGirl(g1, g2)

    println(choose.name)
  }
}
</code></pre><p>第三种方式与第二种方式实现的效果一样，即类与排序类型的耦合以及类与排序规则的耦合，只是 implicitly[Ordering[T]] 要主动引入</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Scala——利用Akka实现的RPC]]></title>
      <url>http://spark8.tech/2016/06/18/scalarpc/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h1><p>使用Scala+Akka实现一个RPC</p>
<h1 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h1><p>先创建一个ActorSystem,单例<br>1.为MasterSystem指定名称,并新建一个名称为Master的Actor<br>2.建一个WorkerSystem,并未WorkerSystem创建一个Actor名字叫Worker<br>3.Worker通过Actor上下文连接Master Actor<br>4.通过返回的ActorSelection 向Master发送注册信息,包括workerid,host,port<br>  消息是一个case class RegisterWorker,走网络,要实现序列化<br>5.接收来自Worker的注册信息,先判断是否存在网络延迟引起的重复注册<br>6.将worker缓存到一个HashMap中,存所有worker的信息<br>  HashSet的workers 是为了第13步方便查找worker的信息<br>7.通知Worker注册成功,并将注册的主机url发送给Woker<br>  由RegisteredWorker传递消息<br>8.Worker接收到Master注册成功的反馈信息<br>  保存主机url以备掉线后重新连接<br>9.发送定时心跳 ，心跳是一个case class,但是schedule方法需要一个ActorRef<br>  而master是ActorSelector不能发送,有一个取巧的办法,发送给自己一个 case object指令(单例,无数据)<br>  其中要导入一个隐式转换，才能启动定时器<br>10.发送心跳之前要进行一些业务逻辑和检查,确认worker工作正常然后再发送心跳<br>11.Master接收到心跳,从Map里面取出worker的id,更新最近一次心跳时间<br>12.Master启动时就开始检查在preStart启动一个定时器，用于周期检查超时的Worker，(技巧)发消息给自己，也用了一个取巧的方法<br>13.从worker中过滤出间隔时间超过检查周期的workerInfo,再将超时的worker从内存中移除掉</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h3><pre><code>//在Akka中负责监控和创建Actor的老大叫ActorSystem
//负责正在通信的叫Actor
class Master(val masterHost: String, val masterPort: Int) extends Actor {

  val idToWorker = new mutable.HashMap[String, WorkerInfo]()
  val workers = new mutable.HashSet[WorkerInfo]()
  val CHECK_INTERVAL = 15000

  //preStart会被调用一次，在构造方法之后，receive方法之前
  override def preStart(): Unit = {
    //12.Master启动时就开始检查在preStart启动一个定时器，用于周期检查超时的Worker
    //   发消息给自己,也用了一个取巧的方法
    import context.dispatcher
    context.system.scheduler.schedule(0 millis, CHECK_INTERVAL millis, self, CheckTimeOutWorker)
  }

  override def receive: Receive = {
    //Worker -&gt; Master
    case RegisterWorker(id, host, port, memory, cores) =&gt; {
      //5.接收来自Worker的注册信息,先判断是否存在网络延迟引起的重复注册
      if (!idToWorker.contains(id)) {
        val workerInfo = new WorkerInfo(id, host, port, memory, cores)
        //6.将worker缓存到一个HashMap中,存所有worker的信息
        //  HashSet的workers 是为了第13步方便查找worker的信息
        idToWorker += (id -&gt; workerInfo)
        workers += workerInfo
        println(&quot;a worker registered&quot;)
        //7.通知Worker注册成功,并将注册的主机url发送给Woker
        //  由RegisteredWorker传递消息
        sender ! RegisteredWorker(s&quot;akka.tcp://${Master.MASTER_SYSTEM}@$masterHost:$masterPort/user/${Master.MASTER_ACTOR}&quot;)
      }
    }

    case Heartbeat(workerId) =&gt; {
      //11.Master接收到心跳,从Map里面取出worker的id,更新最近一次心跳时间
      val workerInfo = idToWorker(workerId)
      val current_time = System.currentTimeMillis()
      //更新最后一次心跳时间
      workerInfo.lastHeartbearTime = current_time
    }

    case CheckTimeOutWorker =&gt; {
      //13.从worker中过滤出间隔时间超过检查周期的workerInfo,再将超时的worker从内存中移除掉
      val currentTime = System.currentTimeMillis()
      val toRemove: mutable.HashSet[WorkerInfo] = workers.filter(w =&gt; currentTime - w.lastHeartbearTime &gt; CHECK_INTERVAL)
      toRemove.foreach(deadWorker =&gt; {
        //将超时的worker从内存中移除掉
        idToWorker -= deadWorker.id
        workers -= deadWorker
      })
      println(&quot;num of workers &quot; +  workers.size)
    }

  }
}

object Master {

  val MASTER_SYSTEM = &quot;MasterSystem&quot;
  val MASTER_ACTOR = &quot;Master&quot;

  def main(args: Array[String]) {
    //如果想创建Actor，必须先创建他的老大，ActorSystem（单例的）

    val host = args(0)
    val port = args(1).toInt
    val configStr =
      s&quot;&quot;&quot;
         |akka.actor.provider = &quot;akka.remote.RemoteActorRefProvider&quot;
         |akka.remote.netty.tcp.hostname = &quot;$host&quot;
         |akka.remote.netty.tcp.port = &quot;$port&quot;
         &quot;&quot;&quot;.stripMargin
    val config = ConfigFactory.parseString(configStr)
    //先创建一个ActorSystem,单例
    //1.为MasterSystem指定名称,并新建一个名称为Master的Actor
    val actorSystem = ActorSystem(MASTER_SYSTEM, config)
    actorSystem.actorOf(Props(new Master(host, port)), MASTER_ACTOR)
    actorSystem.awaitTermination()

  }
}
</code></pre><h3 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h3><pre><code>class Worker(val host: String, val port: Int, val masterHost: String, val masterPort: Int, val memory: Int, val cores: Int) extends Actor{

  val worker_id = UUID.randomUUID().toString
  var masterUrl: String = _
  val HEARTBEAT_INTERVAL = 10000
  var master : ActorSelection = _


  override def preStart(): Unit = {
    //3.Worker通过Actor上下文连接Master Actor
    master = context.actorSelection(s&quot;akka.tcp://${Master.MASTER_SYSTEM}@$masterHost:$masterPort/user/${Master.MASTER_ACTOR}&quot;)
    //4.通过返回的ActorSelection 向Master发送注册信息,包括workerid,host,port
    //  消息是一个case class RegisterWorker,走网络,要实现序列化
    master ! RegisterWorker(worker_id, host, port, memory, cores)
  }

  override def receive: Receive = {
    //8.Worker接收到Master注册成功的反馈信息
    case RegisteredWorker(masterUrl) =&gt; {
      //保存主机url以备重新连接
      this.masterUrl = masterUrl
      //9.发送定时心跳 ，心跳是一个case class,但是schedule方法需要一个ActorRef
      //  而master是ActorSelector不能发送,有一个取巧的办法,发送给自己一个 case object指令(单例,无数据)
      //  导入一个隐式转换，才能启动定时器
      import context.dispatcher
      context.system.scheduler.schedule(0 millis, HEARTBEAT_INTERVAL millis, self, SendHeartbeat)
    }

    case SendHeartbeat =&gt; {
      //10.发送心跳之前要进行一些业务逻辑和检查,确认worker工作正常
      //   然后再发送心跳
      master ! Heartbeat(worker_id)
    }
  }
}

object Worker {

  val WORKER_SYSTEM = &quot;WorkerSystem&quot;
  val WORKER_ACTOR = &quot;Worker&quot;

  def main(args: Array[String]) {

    val host = args(0)
    val port = args(1).toInt
    val masterHost = args(2)
    val masterPort = args(3).toInt
    val memory = args(4).toInt
    val cores = args(5).toInt

    val configStr =
      s&quot;&quot;&quot;
         |akka.actor.provider = &quot;akka.remote.RemoteActorRefProvider&quot;
         |akka.remote.netty.tcp.hostname = &quot;$host&quot;
         |akka.remote.netty.tcp.port = &quot;$port&quot;
         &quot;&quot;&quot;.stripMargin
    val config = ConfigFactory.parseString(configStr)
    val actorSystem = ActorSystem(WORKER_SYSTEM, config)
    //2.建一个WorkerSystem,并未WorkerSystem创建一个Actor名字叫Worker
    actorSystem.actorOf(Props(new Worker(host, port, masterHost, masterPort, memory, cores)), WORKER_ACTOR)
    actorSystem.awaitTermination()

  }
}
</code></pre><h3 id="WorkerInfo"><a href="#WorkerInfo" class="headerlink" title="WorkerInfo"></a>WorkerInfo</h3><pre><code>/**
  * 用于封装Worker的信息
  * @param id
  * @param host
  * @param port
  * @param memory
  * @param cores
  */

class WorkerInfo(val id: String, val host: String, val port: Int, val memory: Int, val cores: Int) {

  var lastHeartbearTime : Long = _
}
</code></pre><h3 id="RemoteMsg"><a href="#RemoteMsg" class="headerlink" title="RemoteMsg"></a>RemoteMsg</h3><pre><code>trait RemoteMsg extends Serializable

//Worker -&gt; Master 的case class,内容为worker信息,继承RemoteMsg
case class RegisterWorker(id: String, host: String, port: Int, memory: Int, cores: Int) extends RemoteMsg

case class Heartbeat(workerId: String) extends RemoteMsg

//Master -&gt; Worker 的case class,消息内容为主机的url,继承RemoteMsg
case class RegisteredWorker(masterUrl: String) extends RemoteMsg

//Worker -&gt; self
case object SendHeartbeat

//Master -&gt; self
case object CheckTimeOutWorker
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Scala—-有序列表合并]]></title>
      <url>http://spark8.tech/2016/06/18/scalayx/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<p>算法本身并不复杂，但是要注意的是递归一定要显式的声明返回值类型。</p>
<pre><code>  object SortList {

  def main(args: Array[String]) {

    //合并算法
    def mergedsort[T] (less: (T,T) =&gt; Boolean)(input : List[T]):List[T] = {
      /**
       * @param  xlist 要合并的有序列表
       * @param  ylist 要合并的有序列表
       * @return 合并后的列表
       */
      def merge(xlist: List[T], ylist: List[T]): List[T] = {
        (xlist,ylist) match{
          case (Nil,_) =&gt; ylist
          case (_,Nil) =&gt; xlist
          case (x :: xtail,y :: ytail) =&gt;
            if(less(x,y))  x :: merge(xtail,ylist)
            else y :: merge(xlist,ytail)
        }
      }
      val n =input.length / 2
      if(n == 0) input
      else{
        val (x,y) =input splitAt n
        merge(mergedsort(less)(x), mergedsort(less)(y))

      }
    }

    println(mergedsort((x : Int,y :Int) =&gt; x &lt; y) (List(1,5,4,7,235,333,43,2,6)))

    val reverse_SortList = mergedsort((x:Int,y:Int) =&gt; x &gt; y ) _
    println(reverse_SortList(List(1,5,4,7,235,333,43,2,6)))

  }
}
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[日志监控告警系统（二）]]></title>
      <url>http://spark8.tech/2016/05/18/rizhijiankong2/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<p>#基于实时日志监控业务的一些考虑<br>Q: 业务对于时效的要求，要怎样的采集方式？</p>
<blockquote>
<p>A:  实时采集不断读取各日志服务器日志</p>
</blockquote>
<p>Q: 日志数据的来源有哪些，数据量有多大？</p>
<blockquote>
<p>A:  日志来自8个业务系统(订单系统、商品系统等)，假设各系统每天产生的日志总和为1T。那么平均20M/s，晚上少流量，正常40M/s，类似双11促销峰值10倍400M/s,不考虑未来有可能数据增加到10T，仅以当下数据量来说。设计目标是使用大量低速高容量的硬盘短时间内存储更多的数据。而单机硬盘默认2T，增加2T硬盘使得每个机器达到4T。1T的数据保存5天，备份一份，就是10T空间，基本上3台机器可以满足。<br>就经验上来说，对于小于10T的数据量，3台4T的机器足以，不要过度设计和考虑容量问题，毕竟日志的存储天数可以调的，要考虑成本。</p>
</blockquote>
<p>Q：如何实现对日志的清洗、过滤处理？</p>
<blockquote>
<p>A：日志来自8个系统的日志，存储在专门的日志服务器集群中，每个系统都有一个flume的agent，source中添加拦截器，区分来自各个系统的aped，然后sink到kafka集群。LogMonitorTopology通过于flume sink相同的zk配置从Kafka-spout获取外部数据源。FileterBolt调用message.parse方法对消息进行合法性检验，<br><img src="/images/messageparse.png" alt="" title="messageparse"></p>
</blockquote>
<p>Q：过滤后的数据是为了实现怎样的业务目的，要匹配怎样的处理规则？</p>
<blockquote>
<p>A：根据各个appid得知此消息属于哪个业务系统，从mysq中取出该系统对应的ruleList缓存至worker进程，用消息去匹配规则列表，匹配成功则向该系统管理员发送邮件或者短信。而为什么在worker进程里面都保存一份而不是存redis全局共享一份，是因为每个进程内都条记录处理都需要pull一次，有大量的进程间通信，消耗性能。<br><img src="/images/messagetrigger.png" alt="" title="messagetrigger"></p>
</blockquote>
<p>Q：错误信息经常是好几条日志信息一起出现的，怎么解决重复发送邮件的问题？</p>
<blockquote>
<p>A:  业务就是为了保证不重复发送邮件通知，基于业务本身，将appid+触发的ruleid生成唯一的key存入redis，每次发送消息前先去redis中检查，因为只有极少的日志携带规则的信息，所以此处用redis没问题。<br><img src="/images/messageexe.png" alt="" title="messagecf"></p>
</blockquote>
<p>Q:  集群模式下如何保证数据匹配的规则的数据一致性？</p>
<blockquote>
<p>A:  程序启动时初始化规则列表，并采用定时更新规则数据的策略，但定时任务在运行时必涉及线程安全（如2个worker进程，10个Bolt的并行度，则每个worker中必有5个bolt task运行）通过加锁和开关实现线程安全控制。<br><img src="/images/messagebxd.png" alt="规则线程安全图" title="messagebxd"><br><img src="/images/messagekgs.png" alt="" title="messagekgs"></p>
</blockquote>
<p>Q：最后，在实现了各个环节的功能后，如何优化整体性能？</p>
<blockquote>
<p>A：整个实时日志监控项目的后台部分由flume+kafka+storm+redis+mysql组成。其中性能优化的组件主要在flume、kafka和storm。<br>第一是规范数据来源，此处应用到flume日志采集时自定义拦截器，使得日志在采集阶段即已分类，方便storm的数据处理。<br>第二是提高数据吞吐量，此处关键是kafka分片数的确定，决定分片数的主要因素是数据量，理论上kafka吞吐数据能达到600M/S，我们以300M/S来计算，10T数据需要14个分片，这也间接的决定了storm分配的spout并行度为14个。<br>第三是数据处理效率，在前面已经优化的基础上，storm处理的复杂度已大为降低，同一Topology内设置spout数等于kafka分片数，经过aped分组后，将目的bolt按字段分组(field grouping或者localorfieldgrouping)，并将bolt的executor数量设置为分组数的倍数。同一worker内 读取规则库用本地缓存减少了进程间通信次数可能带来的延迟。通过这两个措施即可提高storm的处理效率。</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[日志监控告警系统（一）]]></title>
      <url>http://spark8.tech/2016/05/17/rizhijiankong1/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>随着公司业务发展，支撑公司业务的各种系统越来越多，为了保证公司的业务正常发展，急需要对这些线上系统的运行进行监控，做到问题的及时发现和处理，最大程度减少对业务的影响。<br>目前系统分类有：<br>1)    有基于Tomcat的web应用<br>2)    有独立的Java Application应用<br>3)    有运行在linux上的脚本程序<br>4)    有大规模的集群框架（zookeeper、Hadoop、Storm…）<br>5)    有操作系统的运行日志<br>主要功能需求分为：<br>监控系统日志中的内容，按照一定规则进行过滤<br>发现问题之后通过短信和邮件进行告警</p>
<h2 id="功能分析"><a href="#功能分析" class="headerlink" title="功能分析"></a>功能分析</h2><p>数据输入<br>使用flume客户端获取个系统的数据；<br>用户通过页面输入系统名称、负责人触发规则等信息<br>数据存储<br>使用flume采集数据并存放在kafka集群中<br>数据计算<br>使用storm编写程序对日志进行过滤，将满足过滤规则的信息，通过邮件短信告警并保存到数据库中<br>数据展示<br>管理页面可以查看触发规则的信息，系统负责人，联系方式，触发信息明细等</p>
<h2 id="原型设计"><a href="#原型设计" class="headerlink" title="原型设计"></a>原型设计</h2><p>产品经理设计原型</p>
<h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><h3 id="整体架构设计"><a href="#整体架构设计" class="headerlink" title="整体架构设计"></a>整体架构设计</h3><p><img src="/images/DraggedImage.png" alt="整体架构"><br><img src="/images/ztlc.png" alt="整体流程图" title="整体流程图"><br>主要架构为应用+flume+kafka+storm+mysql+Java web。数据流程如下：</p>
<ol>
<li>应用程序使用log4j产生日志</li>
<li>部署flume客户端监控应用程序产生的日志信息，并发送到kafka集群中</li>
<li>storm spout拉去kafka的数据进行消费，逐条过滤每条日志的进行规则判断，对符合规则的日志进行邮件告警。</li>
<li>最后将告警的信息保存到mysql数据库中，用来进行管理。</li>
</ol>
<h3 id="Flume设计"><a href="#Flume设计" class="headerlink" title="Flume设计"></a>Flume设计</h3><p>Flume说明<br>Flume是一个分布式、可靠地、可用的服务，用来收集、聚合、传输日志数据。<br>它是一个基于流式数据的架构，简单而灵活。具有健壮性、容错机制、故障转移、恢复机制。<br>它提供一个简单的可扩展的数据模型，容许在线分析程序。F<br>Flume 作为 cloudera 开发的实时日志收集系统，受到了业界的认可与广泛应用。<br>Flume 设计摘要<br>使用 Flume EXEC执行一个linux命令来生成数据源。例如，可以用tail命令监控一个文件，那么，只要文件增加内容，EXEC就可以将增加的内容作为数据源发送出去。<br>使用 org.apache.flume.plugins.KafkaSink，将Flume EXEC产生的数据源发送到Kafka中。</p>
<h3 id="Kafka设计"><a href="#Kafka设计" class="headerlink" title="Kafka设计"></a>Kafka设计</h3><p>Kafka说明<br>kafka是一个分布式消息队列：生产者、消费者的功能。<br>Kakfa设计摘要<br>部署kafka集群，在集群中添加一个Topic：monitor_realtime_javaxy</p>
<h3 id="Storm设计"><a href="#Storm设计" class="headerlink" title="Storm设计"></a>Storm设计</h3><p>KafkaSpout读取数据，需要配置Topic：monitor_realtime_javaxy<br>FilterBolt判断规则<br>NotifyBolt用来发送邮件或短信息<br>Save2DB用来将告警信息写入mysql数据库</p>
<h3 id="数据模型设计"><a href="#数据模型设计" class="headerlink" title="数据模型设计"></a>数据模型设计</h3><h4 id="用户表"><a href="#用户表" class="headerlink" title="用户表"></a>用户表</h4><p>用来保存用户的信息，包括账号、手机号码、邮箱、是否有效等信息。各字段请参加<a href="github">github</a>具体bean(comment截屏不便)<br><img src="/images/log_monitor_user.png" alt="log_monitor_user" title="log_monitor_user"></p>
<h4 id="应用表"><a href="#应用表" class="headerlink" title="应用表"></a>应用表</h4><p>用来保存应用的信息，包括应用名称、应用描述、应用是否在线等信息。<br><img src="/images/log_monitor_app.png" alt="log_monitor_app" title="log_monitor_app"></p>
<h4 id="应用类型表"><a href="#应用类型表" class="headerlink" title="应用类型表"></a>应用类型表</h4><p>用来保存应用的类型等信息。<br><img src="/images/log_monitor_app_type.png" alt="log_monitor_app\_type" title="log_monitor_app_type"></p>
<h4 id="规则表"><a href="#规则表" class="headerlink" title="规则表"></a>规则表</h4><p>用来保存规则的信息，包括规则名称，规则描述，规则关键词等信息。<br><img src="/images/log_monitor_rule.png" alt="log_monitor_rule" title="log_monitor_rule"></p>
<h4 id="规则记录表"><a href="#规则记录表" class="headerlink" title="规则记录表"></a>规则记录表</h4><p>用来保存触发规则后的记录，包括告警编号、是否短信告知、是否邮件告知、告警明细等信息。<br><img src="/images/log_monitor_rule_record.png" alt="log_monitor_rule_record" title="log_monitor_rule_record"></p>
<h2 id="代码开发"><a href="#代码开发" class="headerlink" title="代码开发"></a>代码开发</h2><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><p><img src="/images/ssgc.png" alt="工程结构" title="工程结构"></p>
<h4 id="LogMonitorTopologyMain驱动类"><a href="#LogMonitorTopologyMain驱动类" class="headerlink" title="LogMonitorTopologyMain驱动类"></a>LogMonitorTopologyMain驱动类</h4><p><img src="/images/logmtm.png" alt=""></p>
<h4 id="KafkaSpout获取数据源"><a href="#KafkaSpout获取数据源" class="headerlink" title="KafkaSpout获取数据源"></a>KafkaSpout获取数据源</h4><p><img src="/images/kafkasp.png" alt=""></p>
<h4 id="FilterBolt过滤日志信息"><a href="#FilterBolt过滤日志信息" class="headerlink" title="FilterBolt过滤日志信息"></a>FilterBolt过滤日志信息</h4><p><img src="/images/filb.png" alt=""></p>
<h4 id="PrepareRecordBolt发送邮件告警和短信告警"><a href="#PrepareRecordBolt发送邮件告警和短信告警" class="headerlink" title="PrepareRecordBolt发送邮件告警和短信告警"></a>PrepareRecordBolt发送邮件告警和短信告警</h4><p><img src="/images/prep.png" alt=""></p>
<h4 id="SaveMessage2MySq保存到数据库"><a href="#SaveMessage2MySq保存到数据库" class="headerlink" title="SaveMessage2MySq保存到数据库"></a>SaveMessage2MySq保存到数据库</h4><p><img src="/images/s2m.png" alt=""></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Shell编程——使用HDFS Shell上传文件]]></title>
      <url>http://spark8.tech/2016/05/17/shelltohdfs/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>点击流日志每天都10G，需要上传数据仓库（Hadoop HDFS）上</p>
<h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>一般上传文件都是在凌晨24点操作，由于很多种类的业务数据都要在晚上进行传输，为了减轻服务器的压力，避开高峰期。需要伪实时的上传，即当文件有10G的时候，就上传一个。<br>一、    正常上传到hdfs的需求</p>
<ol>
<li>上传到hdfs的文件需要校验完整性。MD5</li>
<li>一个文件在上传过程中另一个进程不能再次上传该文件  <em>COPY</em></li>
<li>上传到hdfs的文件名不能相同   时间戳</li>
<li>定时上传文件到hdfs  PUT<br>二、    异常上传的需求</li>
<li><p>文件上传到hdfs过程中失败，需要有重试机制，<br> 若能再次连上需要保证数据能再次上传且数据不重复，也不丢失，<br> HDFS文件列表和本地待上传的文件列表进行比对<br> grep -diff<br> 若超过重试限制后需要发短信通知<br> java -jar </p>
</li>
<li><p>ftp服务连不上，需要有重试机制，超过重试限制需要发短信通知</p>
</li>
<li>运行脚本的服务器挂了，需要有备份服务器启动，并发短信通知<h2 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h2><h3 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h3></li>
<li>HDFS SHELL:  hadoop fs  –put   xxxx.tar  /data    还可以使用 Java Api<br>  满足上传一个文件，不能满足定时、周期性传入。</li>
<li><p>Linux crontab<br>“crontab -e <em>/5 </em> <em> </em> * $home/bin/command.sh ” //五分钟执行一次<br>系统会自动执行脚本，每五分钟一次，执行时判断文件是否等于10G，如果等于10G就可以上传。</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>以下为正常上传部分代码，异常上传部分未实现，只提供如上思路</p>
<pre><code>!/bin/bash
set java env
export JAVA_HOME=/export/servers/jdk
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH

set hadoop env
export HADOOP_HOME=/export/servers/hadoop
export PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH
</code></pre><p>日志文件存放的目录</p>
<pre><code>log_src_dir=/export/software/

s=du -k $log_src_dir |awk &apos;{print $1}&apos;
if [ $s -lt 1024000000 ]
then
exit 1
else
continue
fi
</code></pre><p>待上传文件存放的目录</p>
<pre><code>log_appending_dir=/export/data/click_log/
</code></pre><p>日志文件上传到hdfs的根路径</p>
<pre><code>hdfs_root_dir=/data/clickLog/20151226/
</code></pre><p>读取日志文件的目录，判断是否有需要上传的文件</p>
<pre><code>ls $log_src_dir | while read fileName
do
if [ &quot;hadoop.log1&quot; = &quot;$fileName&quot; ];then
date=date +%Y_%m_%d_%H_%M_%S
mv $log_src_dir$fileName $log_appending_dir&quot;xxxxx_click_log_&quot;$date
echo $log_appending_dir&quot;xxxxx_click_log_&quot;$date &gt;&gt; /export/data/click_log/willDoing.$date
fi

done
</code></pre><p>过滤掉正在copy和已经copy的文件</p>
<pre><code>ls $log_appending_dir | grep will |grep -v &quot;_COPY_&quot; | grep -v &quot;_DONE_&quot; | while read line
do
</code></pre><p>对拿到的文件重命名</p>
<pre><code>mv $log_appending_dir$line $log_appending_dir$line&quot;_COPY_&quot;
cat $log_appending_dir$line&quot;_COPY_&quot; |while read line
do
hadoop fs -put $line $hdfs_root_dir
done    
mv $log_appending_dir$line&quot;_COPY_&quot;  $log_appending_dir$line&quot;_DONE_&quot;
done
</code></pre><p>重试核心代码</p>
<pre><code>function failOver(){
    echo &quot;重试机制启动....&quot;
    #转钟逻辑处理
    if [ &quot;00:00&quot; = $tag ] || [ &quot;00:01&quot; = $tag ]
    then
    yesterday_dt=date --date=&apos;yesterday&apos; +%Y-%m-%d
    yesterday_ftp_date=date --date=&apos;yesterday&apos; +%Y%m%d
    put_local_file_to_hdfs $yesterday_dt $yesterday_ftp_date
    fi
    #半小时逻辑处理
    if [ &quot;40&quot; = $tagM ] || [ &quot;41&quot; = $tagM ]
    then
            today_dt=date +%Y-%m-%d
            today_ftp_date=date +%Y%m%d
            put_local_file_to_hdfs $today_dt $today_ftp_date
    fi
    echo &quot;重试机制执行完毕...&quot;
}

function put_local_file_to_hdfs(){

    diffFile=$log_dir&quot;differ_file&quot;$executeTime.log
    hdfs_file_list=$log_dir&quot;hdfs_file_list&quot;$executeTime.log
    local_file_list=$log_dir&quot;local_file_list&quot;$executeTime.log

    hadoop fs -ls /apps/hive/warehouse/stage.db/$hdfs_table/dt=$1/ | awk &apos;{print $8}&apos;|while read line
    do
            file_name=${line##*/}
            suffix=${file_name##*.}
            #如果文件正在上传中，视为已经上传成功
            if [ &quot;_COPYING_&quot; = &quot;$suffix&quot; ]
            then
                    echo &quot;此文件正在上传：&quot;$line
                    file_name=${file_name%.*}
            fi
            echo $file_name &gt;&gt; $hdfs_file_list
    done

    ls $tmp_Dir$2 |grep $file_prefix &gt;$local_file_list

    grep -vxFf $hdfs_file_list $local_file_list &gt; $diffFile

    rm $hdfs_file_list
    rm $local_file_list

    file_size=cat $diffFile |wc -l
    echo &quot;当前重试的文件个数：&quot;$file_size
    if (( &quot;$file_size&quot; &gt;= &quot;1&quot; ))
    then
            #如果重试的文件大于，触发短信告警信息
            #source ~/.base_profile
            /soft/java/bin/java -jar /export/server/real_platform/sendmsg/sendmsg.jar &quot;15652306418,18211153576&quot; $hostname&quot;机器下有 &quot;$file_size&quot; 个文件正在重试上传，小偷程序遇到问题了，请查看！&quot;
            echo &quot;发送短信成功！&quot;
    fi

    if [ -f $diffFile ]
    then 
            cat $diffFile |while read line
                            do
                            tarFile=$tmp_Dir$2&quot;/&quot;$line
                            echo &quot;开始上传文件：&quot;$tarFile
                            echo &quot;hadoop命令：hadoop fs -put &quot;$tarFile &quot;/apps/hive/warehouse/stage.db/&quot;$hdfs_table&quot;/dt=&quot;$1&quot;/&quot;
                            hadoop fs -put $tarFile /apps/hive/warehouse/stage.db/$hdfs_table/dt=$1/
                            echo &quot;上传文件结束：&quot;$tarFile
                            done
            rm $diffFile 
    fi
}
</code></pre></li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux——网络连接三种方式]]></title>
      <url>http://spark8.tech/2016/05/16/Linuxline/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="一、NAT"><a href="#一、NAT" class="headerlink" title="一、NAT"></a>一、NAT</h2><p><img src="/images/NAT.png" alt="NAT示意图" title="NAT连接"></p>
<h2 id="二、bridge"><a href="#二、bridge" class="headerlink" title="二、bridge"></a>二、bridge</h2><p><img src="/images/bridge.png" alt="bridge示意图"></p>
<h2 id="三、host-only"><a href="#三、host-only" class="headerlink" title="三、host-only"></a>三、host-only</h2><p><img src="/images/host-only.png" alt="host-only示意图" title="host-only"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Shell编程——自动化软件部署脚本]]></title>
      <url>http://spark8.tech/2016/05/15/shellautodeploy/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>开发一个脚本，实现对局域网中的N台节点批量自动下载、安装jdk</p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>编写一个启动脚本，发送一个软件安装脚本到每一台机器<br>然后启动每台机器上的软件安装脚本来执行软件下载和安装<br><img src="/images/自动化软件部署脚本思路图.png" alt="自动化软件部署脚本思路图" title="自动化软件部署脚本思路图"></p>
<h2 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h2><p>难点：使用scp命令远程拷贝文件时，会有人机交互的过程，如何让脚本完成人机交互?<br>解决：expect<br>用法示例：先观察  ssh localhost 的过程，再看expect的功能</p>
<pre><code>#!/bin/bash/expect
   # exp_test.sh
   set timeout -1;
   spawn ssh localhost;
   expect {
       &quot;(yes/no)&quot; {send &quot;yes\r&quot;;exp_continue;}
       &quot;password:&quot; {send &quot;hadoop\r&quot;;exp_continue;}
       eof        {exit 0;}
   }
</code></pre><h2 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h2><p>选择一台服务器（比如CentOS）作为软件源服务器</p>
<ol>
<li>安装httpd</li>
<li>制作局域网yum源</li>
<li>编写repo配置</li>
<li>分发repo配置到局域网</li>
<li>准备一个jdk安装包放在内网web服务器上</li>
</ol>
<h2 id="脚本开发"><a href="#脚本开发" class="headerlink" title="脚本开发"></a>脚本开发</h2><ol>
<li><p>启动脚本boot.sh</p>
<pre><code>#!/bin/bash  
SERVERS=&quot;mini3&quot; 
PASSWORD=hadoop 
BASE_SERVER=192.168.59.101 
auto_ssh_copy_id() { 
    expect -c &quot;set timeout -1;
    spawn ssh-copy-id $1;
    expect {
        *(yes/no)* {send -- yes\r;exp_continue;}
        *assword:* {send -- $2\r;exp_continue;}
        eof{exit 0;}
    }&quot;;
}
ssh_copy_id_to_all() {
    for SERVER in $SERVERS
        do
        auto_ssh_copy_id $SERVER $PASSWORD
        done
    }
ssh_copy_id_to_all 
</code></pre></li>
<li><p>执行脚本install.sh</p>
<pre><code>#!/bin/bash
BASE_SERVER=192.168.59.101

yum install -y wget

wget $BASE_SERVER/soft/jdk-7u67-linux-x64.gz

tar -zxvf jdk-7u67-linux-x64.gz -C /usr/local

cat &gt;&gt; /etc/profile &lt;&lt; EOF
export JAVA_HOME=/usr/local/jdk1.7.0_67
export PATH=\$PATH:\$JAVA_HOME/bin
EOF

source /etc/profile
</code></pre></li>
<li><p>执行脚本</p>
</li>
</ol>
<p>只要在baseServer即mini上启动boot.sh即可</p>
<h2 id="通用问题"><a href="#通用问题" class="headerlink" title="通用问题"></a>通用问题</h2><p>Q1.目标机器名需要写死在脚本中<br>Tips：可以将所有需要安装软件的机器名写在一个文件：比如slaves中让脚本自动读取slaves文件中的机器名来批量安装</p>
<pre><code>cat slaves | while read host
do
echo $host
expect -c &quot;set timeout -f
spawn ssh-copy-id $host&quot;
done
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[轻量级RPC框架（二）]]></title>
      <url>http://spark8.tech/2016/05/12/easyrpc2/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>此RPC用于Action远程调用Service，用到的技术点包括NIO通信+序列化+反射(Spring)+动态代理(Proxy类)+Zookeeper<br>设计思路如下图：<br><img src="/images/RPC%E6%A1%86%E6%9E%B6.png" alt="RPC框架" title="RPC框架"></p>
<h2 id="工程实现"><a href="#工程实现" class="headerlink" title="工程实现"></a>工程实现</h2><p>工程目录如下：<br><img src="/images/%E5%B7%A5%E7%A8%8B%E7%9B%AE%E5%BD%95.png" alt="工程目录" title="工程目录"><br>具体实现及项目间依赖请参见<a href="www.baidu.com">github链接</a>，此处只做各模块功能说明</p>
<h3 id="rpc-服务端"><a href="#rpc-服务端" class="headerlink" title="rpc-服务端"></a>rpc-服务端</h3><h4 id="1-rpc-sample-server模块"><a href="#1-rpc-sample-server模块" class="headerlink" title="1.rpc-sample-server模块"></a>1.rpc-sample-server模块</h4><p> HelloServiceImpl<br>  接口的实现类</p>
<p> RpcBootstrap<br>  用户系统服务端的启动入口其意义是启动springcontext，从而构造框架中的RpcServer<br>  亦即：将用户系统中所有标注了RpcService注解的业务发布到RpcServer中</p>
<h4 id="2-rpc-server模块"><a href="#2-rpc-server模块" class="headerlink" title="2.rpc-server模块"></a>2.rpc-server模块</h4><p> RpcHandler<br>  处理具体的业务调用<br>  通过构造时传入的“业务接口及实现”handlerMap，来调用客户端所请求的业务方法<br>  并将业务方法返回值封装成response对象写入下一个handler（即编码handler—— RpcEncoder）</p>
<p> RpcServer<br>  框架的RPC 服务器（用于将用户系统的业务类发布为 RPC 服务）<br>  使用时可由用户通过spring-bean的方式注入到用户的业务系统中<br>  由于本类实现了ApplicationContextAware InitializingBeans，spring构造本对象时会调用setApplicationContext()方法，从而可以在方法中通过自定义注解获得用户的业务接口和实现，还会调用afterPropertiesSet()方法，在方法中启动netty服务器</p>
<p>RpcService<br>  RPC 请求注解（标注在服务实现类上</p>
<h4 id="3-rpc-registry模块"><a href="#3-rpc-registry模块" class="headerlink" title="3.   rpc-registry模块"></a>3.   rpc-registry模块</h4><p> Constant<br>  zookeeper 常量</p>
<p> ServiceDiscovery<br>  用于client发现server节点的变化 ，实现负载均衡）</p>
<p>ServiceRegistry<br>  服务注册 ，ZK 在该架构中扮演了“服务注册表”的角色，用于注册所有服务器的地址与端口，并对客户端提供服务发现的功能</p>
<h4 id="4-rpc-common-server模块"><a href="#4-rpc-common-server模块" class="headerlink" title="4.  rpc-common-server模块"></a>4.  rpc-common-server模块</h4><p>RpcDecoder<br>  解码器<br>RpcEncoder<br>  编码器<br>RpcRequest<br>  封装 RPC 请求 ，封装发送的object的反射属性<br>RpcResponse<br>  封装 RPC 响应 ，封装相应object<br>SerializationUtil<br>  基于  Protostuff  实现的序列化工具类</p>
<h4 id="5-rpc-common-interfaces模块"><a href="#5-rpc-common-interfaces模块" class="headerlink" title="5. rpc-common-interfaces模块"></a>5. rpc-common-interfaces模块</h4><p>Entity<br>  封装的通信实体对象类<br>HelloService<br>  客户端调用的接口类</p>
<h3 id="rpc-客户端"><a href="#rpc-客户端" class="headerlink" title="rpc-客户端"></a>rpc-客户端</h3><h4 id="1-rpc-sample-client模块"><a href="#1-rpc-sample-client模块" class="headerlink" title="1.rpc-sample-client模块"></a>1.rpc-sample-client模块</h4><p>HelloActionTest<br>  客户端发起调用的Action类</p>
<h4 id="2-rpc-client模块"><a href="#2-rpc-client模块" class="headerlink" title="2.rpc-client模块"></a>2.rpc-client模块</h4><p>RpcClient<br>  框架的RPC 客户端（用于发送 RPC 请求）<br>RpcProxy<br>  RPC 代理（用于创建 RPC 服务代理）</p>
<h4 id="3-rpc-registry模块-1"><a href="#3-rpc-registry模块-1" class="headerlink" title="3.rpc-registry模块"></a>3.rpc-registry模块</h4><p>  客户端注册zk服务，同Server端</p>
<h4 id="4-rpc-common-client模块"><a href="#4-rpc-common-client模块" class="headerlink" title="4.  rpc-common-client模块"></a>4.  rpc-common-client模块</h4><p>  工具类，同Server端</p>
<h4 id="5-rpc-common-interface模块"><a href="#5-rpc-common-interface模块" class="headerlink" title="5.  rpc-common-interface模块"></a>5.  rpc-common-interface模块</h4><p>  接口类与实体类，同Server端</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[轻量级RPC框架（一）]]></title>
      <url>http://spark8.tech/2016/05/12/easyrpc/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="一、RPC原理"><a href="#一、RPC原理" class="headerlink" title="一、RPC原理"></a>一、RPC原理</h2><ol>
<li><p>消息框架MQ与RPC框架区别<br>RPC（Remote Procedure Call Protocol）是面向动作的，请求响应模式。多用于系统Action层和Service层通信，缓解服务端多线程并发压力。<br>技术组成包括 NIO+序列化+反射+动态代理<br>MQ 是面向数据的，生产者消费者模式，多用于不同系统间的数据传送，面向多订阅用户的消息推送，NIO+序列化+JMS</p>
</li>
<li><p>什么是RPC<br>RPC（Remote Procedure Call Protocol）——远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。<br>RPC采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。首先，客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息。在服务器端，进程保持睡眠状态直到调用信息到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。</p>
</li>
<li>RPC调用过程<br><img src="/images/RPC结构.png" alt="RPC调用过程" title="RPC调用过程"><h2 id="二、NIO原理"><a href="#二、NIO原理" class="headerlink" title="二、NIO原理"></a>二、NIO原理</h2><h3 id="socket-NIO原理"><a href="#socket-NIO原理" class="headerlink" title="socket NIO原理"></a>socket NIO原理</h3></li>
<li>同步与异步，阻塞和非阻塞<br>同步与异步关注的是 消息通信机制<br>同步和异步都是基于应用程序和操作系统处理IO事件所采用的方式<br>阻塞和非阻塞是进程在访问数据的时候，数据是否准备就绪的一种处理方式。<br>阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态<br><a href="https://www.zhihu.com/question/19732473" target="_blank" rel="external">知乎链接</a></li>
<li>NIO原理解读<br>对于网络通信而言，NIO  AIO并没有改变网络通信的基本步骤，只是在原来的基础上（serversocket，socket）做了一个改进。<br><img src="/images/NIO与传统IO.png" alt="NIO与传统IO" title="NIO与传统IO"></li>
</ol>
<h3 id="小结："><a href="#小结：" class="headerlink" title="小结："></a>小结：</h3><p>1）BIO阻塞的IO<br>2）NIO select多路复用+非阻塞   同步非阻塞<br>3）AIO异步非阻塞IO</p>
<h2 id="三、netty框架"><a href="#三、netty框架" class="headerlink" title="三、netty框架"></a>三、netty框架</h2><p>使用实例见<a href="github.com/sparkliwei">github链接</a><br>总结：在使用Handler的过程中，需要注意：<br>1、ChannelInboundHandler之间的传递，通过调用 ctx.fireChannelRead(msg) 实现；调用ctx.write(msg) 将传递到ChannelOutboundHandler。<br>2、ctx.write()方法执行后，需要调用flush()方法才能令它立即执行。<br>3、流水线pipeline中outhandler不能放在最后，否则不生效</p>
<p>如果使用addlast方法来组装handler，则为以下执行顺序：<br>// 注册两个InboundHandler，执行顺序为注册顺序，所以应该是InboundHandler1 InboundHandler2<br>// 注册两个OutboundHandler，执行顺序为注册顺序的逆序，所以应该是OutboundHandler2 OutboundHandler1</p>
<h2 id="四、反射"><a href="#四、反射" class="headerlink" title="四、反射"></a>四、反射</h2><p>常见的是json或字符串文本转换成java对象，见以下实例,此处RPC框架开发中用Spring代替<br>        public class MyReflect {<br>            public String className = null;<br>            @SuppressWarnings(“rawtypes”)<br>            // Class 代表JVM中加载好的一个特定class文件<br>            public Class personClass = null;<br>            /**</p>
<pre><code>     * 反射Person类
     * @throws Exception 
     */
    @Before
    public void init() throws Exception {
        className = &quot;day04.cn.holley_04_reflect.Person&quot;;
        // 通过Class.forName( 类全路径字符串)，就能将这个类的class文件加载到JVM内存中
        personClass = Class.forName(className);
    }
    /**
     *获取某个class文件对象
     */
    @Test
    public void getClassName() throws Exception {
        System.out.println(personClass);
    }
    /**
     *获取某个class文件对象的另一种方式
     */
    @Test
    public void getClassName2() throws Exception {
        System.out.println(Person.class);
    }
    /**
     *创建一个class文件表示的实例对象，底层会调用空参数的构造方法
     */
    @Test
    public void getNewInstance() throws Exception {
        System.out.println(personClass.newInstance());
    }
    /**
     *获取非私有的构造函数
     */
    @SuppressWarnings({ &quot;rawtypes&quot;, &quot;unchecked&quot; })
    @Test
    public void getPublicConstructor() throws Exception {
        Constructor  constructor  = personClass.getConstructor(Long.class,String.class);
        Person person = (Person)constructor.newInstance(100L,&quot;zhangsan&quot;);
        System.out.println(person.getId());
        System.out.println(person.getName());
    }
    /**
     *获得私有的构造函数
     */
    @SuppressWarnings({ &quot;rawtypes&quot;, &quot;unchecked&quot; })
    @Test
    public void getPrivateConstructor() throws Exception {
        Constructor con = personClass.getDeclaredConstructor(String.class);
        con.setAccessible(true);//强制取消Java的权限检测
        Person person2 = (Person)con.newInstance(&quot;zhangsan&quot;);
        System.out.println(&quot;**&quot;+person2.getName());
    }
    /**
     *访问非私有的成员变量
     */
    @SuppressWarnings({ &quot;rawtypes&quot;, &quot;unchecked&quot; })
    @Test
    public void getNotPrivateField() throws Exception {
        Constructor  constructor  = personClass.getConstructor(Long.class,String.class);
        Object obj = constructor.newInstance(100L,&quot;zhangsan&quot;);

        Field field = personClass.getField(&quot;name&quot;);
        field.set(obj, &quot;lisi&quot;);
        System.out.println(field.get(obj));
    }
    /**
     *访问私有的成员变量
     */
    @SuppressWarnings({ &quot;rawtypes&quot;, &quot;unchecked&quot; })
    @Test
    public void getPrivateField() throws Exception {
        Constructor  constructor  = personClass.getConstructor(Long.class);
        Object obj = constructor.newInstance(100L);

        Field field2 = personClass.getDeclaredField(&quot;id&quot;);
        field2.setAccessible(true);//强制取消Java的权限检测
        field2.set(obj,10000L);
        System.out.println(field2.get(obj));
    }
    /**
     *获取非私有的成员函数
     */
    @SuppressWarnings({ &quot;unchecked&quot; })
    @Test
    public void getNotPrivateMethod() throws Exception {
        System.out.println(personClass.getMethod(&quot;toString&quot;));

        Object obj = personClass.newInstance();//获取空参的构造函数
        Method toStringMethod = personClass.getMethod(&quot;toString&quot;);
        Object object = toStringMethod.invoke(obj);
        System.out.println(object);
    }
    /**
     *获取私有的成员函数
     */
    @SuppressWarnings(&quot;unchecked&quot;)
    @Test
    public void getPrivateMethod() throws Exception {
        Object obj = personClass.newInstance();//获取空参的构造函数
        Method method = personClass.getDeclaredMethod(&quot;getSomeThing&quot;);
        method.setAccessible(true);
        Object value = method.invoke(obj);
        System.out.println(value);

    }
    /**
     *
     */
    @Test
    public void otherMethod() throws Exception {
        //当前加载这个class文件的那个类加载器对象
        System.out.println(personClass.getClassLoader());
        //获取某个类实现的所有接口
        Class[] interfaces = personClass.getInterfaces();
        for (Class class1 : interfaces) {
            System.out.println(class1);
        }
        //反射当前这个类的直接父类
        System.out.println(personClass.getGenericSuperclass());
        /**
         * getResourceAsStream这个方法可以获取到一个输入流，这个输入流会关联到name所表示的那个文件上。
         */
        //path 不以’/&apos;开头时默认是从此类所在的包下取资源，以’/&apos;开头则是从ClassPath根下获取。其只是通过path构造一个绝对路径，最终还是由ClassLoader获取资源。
        System.out.println(personClass.getResourceAsStream(&quot;/log4j.properties&quot;));
        System.out.println(personClass.getResourceAsStream(&quot;log4j.properties&quot;));

        //判断当前的Class对象表示是否是数组
        System.out.println(personClass.isArray());
        System.out.println(new String[3].getClass().isArray());

        //判断当前的Class对象表示是否是枚举类
        System.out.println(personClass.isEnum());
        System.out.println(Class.forName(&quot;day04.cn.holley_04_reflect.City&quot;).isEnum());

        //判断当前的Class对象表示是否是接口
        System.out.println(personClass.isInterface());
        System.out.println(Class.forName(&quot;day04.cn.holley_04_reflect.TestInterface&quot;).isInterface());


    }

}
</code></pre><h2 id="五、动态代理"><a href="#五、动态代理" class="headerlink" title="五、动态代理"></a>五、动态代理</h2><h3 id="动态代理：在不修改原业务的基础上，基于原业务方法，进行重新的扩展，实现新的业务。"><a href="#动态代理：在不修改原业务的基础上，基于原业务方法，进行重新的扩展，实现新的业务。" class="headerlink" title="动态代理：在不修改原业务的基础上，基于原业务方法，进行重新的扩展，实现新的业务。"></a>动态代理：在不修改原业务的基础上，基于原业务方法，进行重新的扩展，实现新的业务。</h3><h3 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h3><ol>
<li>旧业务<br>买家调用action，购买衣服，衣服在数据库的标价为50元，购买流程就是简单的调用。</li>
<li>新业务<br>在原先的价格上可以使用优惠券，但是这个功能在以前没有实现过，我们通过代理类，代理了原先的接口方法，在这个方法的基础上，修改了返回值。<br><img src="/images/动态代理proxy.png" alt="动态代理" title="动态代理"><h3 id="实现流程："><a href="#实现流程：" class="headerlink" title="实现流程："></a>实现流程：</h3></li>
<li>书写代理类和代理方法，在代理方法中实现代理Proxy.newProxyInstance</li>
<li>代理中需要的参数分别为：被代理的类的类加载器soneObjectclass.getClassLoader()，被代理类的所有实现接口new Class<a href="#"></a>  Interface.class ，句柄方法new InvocationHandler()</li>
<li>在句柄方法中复写invoke方法，invoke方法的输入有3个参数Object proxy（代理类对象）, Method method（被代理类的方法）,Object<a href="#"></a> args（被代理类方法的传入参数），在这个方法中，我们可以定制化的开发新的业务。</li>
<li><p>获取代理类，强转成被代理的接口<br>最后，我们可以像没被代理一样，调用接口的认可方法，方法被调用后，方法名和参数列表将被传入代理类的invoke方法中，进行新业务的逻辑流程。<br>核心代码：</p>
<pre><code>ProxySaleAction
    /**
     * 什么是动态代理？ 简单的写一个模板接口，剩下的个性化工作，好给动态代理来完成！
     */
    public class ProxySaleAction {

        /**
         *使用代理，在这个代理中，只代理了Boss的yifu方法
         *定制化业务，可以改变原接口的参数、返回值等
         */
        @Test
        public void saleByProxy() throws Exception {
            IBoss boss = ProxyBoss.getProxy(10, IBoss.class, Boss.class);// 将代理的方法实例化成接口
            //IBoss boss = new Boss();// 将代理的方法实例化成接口
            System.out.println(&quot;代理经营！&quot;);

            int money = boss.yifu(&quot;xxl&quot;);// 调用接口的方法，实际上调用方式没有变
            System.out.println(&quot;衣服成交价：&quot; + money);
        }
    }
SaleAction
    public class SaleAction {
        /**
         * 不使用代理，直接调用方法
         * 方法中规定什么业务，就只能调用什么业务，规定什么返回值，就只能输出什么返回值
         */
        @Test
        public void saleByBossSelf() throws Exception {
            IBoss boss = new Boss();
            System.out.println(&quot;老板自营！&quot;);
            int money = boss.yifu(&quot;xxl&quot;);// 老板自己卖衣服，不需要客服，结果就是没有聊天记录
            System.out.println(&quot;衣服成交价：&quot; + money);
        }
    }
ProxyBoss
    public class ProxyBoss {
        /**
         * 对接口方法进行代理
         */
        @SuppressWarnings(&quot;unchecked&quot;)
        public static &lt;T&gt; T getProxy(final int discountCoupon,
                final Class&lt;?&gt; interfaceClass, final Class&lt;?&gt; implementsClass)
                throws Exception {
            return (T) Proxy.newProxyInstance(interfaceClass.getClassLoader(),
                    new Class[] { interfaceClass }, new InvocationHandler() {
                        public Object invoke(Object proxy, Method method,
                                Object[] args) throws Throwable {
                            Integer returnValue = (Integer) method.invoke(
                                    implementsClass.newInstance(), args);// 调用原始对象以后返回的值
                            return returnValue - discountCoupon;
                        }
                    });
        }
    }
</code></pre></li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[推荐系统——User CF vs. Item CF]]></title>
      <url>http://spark8.tech/2016/04/12/tjxt/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="计算复杂度"><a href="#计算复杂度" class="headerlink" title="计算复杂度"></a>计算复杂度</h1><p>Item CF 和 User CF 是基于协同过滤推荐的两个最基本的算法，User CF 是很早以前就提出来了，Item CF 是从 Amazon 的论文和专利发表之后（2001 年左右）开始流行，大家都觉得 Item CF 从性能和复杂度上比 User CF 更优，<strong>其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，同时 也不必频繁更新。但我们往往忽略了这种情况只适应于提供商品的电子商务网站，对于新闻，博客或者微内容的推荐系统，情况往往是相反的，物品的数量是海量 的，同时也是更新频繁的，所以单从复杂度的角度，这两个算法在不同的系统中各有优势，推荐引擎的设计者需要根据自己应用的特点选择更加合适的算法。</strong></p>
<h1 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h1><p>在非社交网络的网站中，内容内在的联系是很重要的推荐原则，它比基于相似用户的推荐原则更加有效。比如在购书网站上，当你看一本书的时候，推荐引擎 会给你推荐相关的书籍，这个推荐的重要性远远超过了网站首页对该用户的综合推荐。可以看到，在这种情况下，Item CF 的推荐成为了引导用户浏览的重要手段。同时 Item CF 便于为推荐做出解释，在一个非社交网络的网站中，给某个用户推荐一本书，同时给出的解释是某某和你有相似兴趣的人也看了这本书，这很难让用户信服，因为用 户可能根本不认识那个人；但如果解释说是因为这本书和你以前看的某本书相似，用户可能就觉得合理而采纳了此推荐。相反的，在现今很流行的社交网络站点中，User CF 是一个更不错的选择，User CF 加上社会网络信息，可以增加用户对推荐解释的信服程度。</p>
<h1 id="推荐多样性和精度"><a href="#推荐多样性和精度" class="headerlink" title="推荐多样性和精度"></a>推荐多样性和精度</h1><p>研究推荐引擎的学者们在相同的数据集合上分别用 User CF 和 Item CF 计算推荐结果，发现推荐列表中，只有 50% 是一样的，还有 50% 完全不同。但是这两个算法确有相似的精度，所以可以说，这两个算法是很互补的。<br>关于推荐的多样性，有两种度量方法：<br>第一种度量方法是从单个用户的角度度量，就是说给定一个用户，查看系统给出的推荐列表是否多样，也就是要比较推荐列表中的物品之间两两的相似度，不 难想到，对这种度量方法，Item CF 的多样性显然不如 User CF 的好，因为 Item CF 的推荐就是和以前看的东西最相似的。<br>第二种度量方法是考虑系统的多样性，也被称为覆盖率 (Coverage)，它是指一个推荐系统是否能够提供给所有用户丰富的选择。在这种指标下，Item CF 的多样性要远远好于 User CF, 因为 User CF 总是倾向于推荐热门的，从另一个侧面看，也就是说，Item CF 的推荐有很好的新颖性，很擅长推荐长尾里的物品。所以，尽管大多数情况，Item CF 的精度略小于 User CF， 但如果考虑多样性，Item CF 却比 User CF 好很多。<br>如果你对推荐的多样性还心存疑惑，那么下面我们再举个实例看看 User CF 和 Item CF 的多样性到底有什么差别。<br>首先，假设每个用户兴趣爱好都是广泛的，喜欢好几个领域的东西，不过每个用户肯定也有一个主要的领域，对这个领域会比其他领域更加关心。<br>给定一个用户，假设他喜欢 3 个领域 A,B,C，A 是他喜欢的主要领域，这个时候我们来看 User CF 和 Item CF 倾向于做出什么推荐：如果用 User CF, 它会将 A,B,C 三个领域中比较热门的东西推荐给用户；而如果用 ItemCF，它会基本上只推荐 A 领域的东西给用户。所以我们看到因为 User CF 只推荐热门的，所以它在推荐长尾里项目方面的能力不足；而 Item CF 只推荐 A 领域给用户，这样他有限的推荐列表中就可能包含了一定数量的不热门的长尾物品，同时 Item CF 的推荐对这个用户而言，显然多样性不足。但是对整个系统而言，因为不同的用户的主要兴趣点不同，所以系统的覆盖率会比较好。<br>从上面的分析，可以很清晰的看到，这两种推荐都有其合理性，但都不是最好的选择，因此他们的精度也会有损失。其实对这类系统的最好选择是，如果系统 给这个用户推荐 30 个物品，既不是每个领域挑选 10 个最热门的给他，也不是推荐 30 个 A 领域的给他，而是比如推荐 15 个 A 领域的给他，剩下的 15 个从 B,C 中选择。所以结合 User CF 和 Item CF 是最优的选择，结合的基本原则就是当采用 Item CF 导致系统对个人推荐的多样性不足时，我们通过加入 User CF 增加个人推荐的多样性，从而提高精度，而当因为采用 User CF 而使系统的整体多样性不足时，我们可以通过加入 Item CF 增加整体的多样性，同样同样可以提高推荐的精度。</p>
<h1 id="用户对推荐算法的适应度"><a href="#用户对推荐算法的适应度" class="headerlink" title="用户对推荐算法的适应度"></a>用户对推荐算法的适应度</h1><p>前面我们大部分都是从推荐引擎的角度考虑哪个算法更优，但其实我们更多的应该考虑作为推荐引擎的最终使用者 – 应用用户对推荐算法的适应度。<br>对于 User CF，推荐的原则是假设用户会喜欢那些和他有相同喜好的用户喜欢的东西，但如果一个用户没有相同喜好的朋友，那 User CF 的算法的效果就会很差，所以一个用户对的 CF 算法的适应度是和他有多少共同喜好用户成正比的。<br>Item CF 算法也有一个基本假设，就是用户会喜欢和他以前喜欢的东西相似的东西，那么我们可以计算一个用户喜欢的物品的自相似度。一个用户喜欢物品的自相似度大，就 说明他喜欢的东西都是比较相似的，也就是说他比较符合 Item CF 方法的基本假设，那么他对 Item CF 的适应度自然比较好；反之，如果自相似度小，就说明这个用户的喜好习惯并不满足 Item CF 方法的基本假设，那么对于这种用户，用 Item CF 方法做出好的推荐的可能性非常低。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Redis应用-LOL盒子英雄数据排行榜]]></title>
      <url>http://spark8.tech/2016/03/14/redis_sortedSet/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>按出场次数多少快速计算LOL中英雄排行榜</p>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><ol>
<li>在redis中需要一个榜单所对应的sortedset数据</li>
<li>玩家每选择一个英雄打一场游戏，就对sortedset数据的相应的英雄分数+1</li>
<li>Lol盒子上查看榜单时，就调用zrange来看榜单中的排序结果<h2 id="代码开发"><a href="#代码开发" class="headerlink" title="代码开发"></a>代码开发</h2></li>
<li><p>LolBoxPlayer</p>
<pre><code>public class LolBoxPlayer {
    public static void main(String[] args) throws Exception {

        Jedis jedis = new Jedis(&quot;192.168.59.101&quot;, 6379);

        Random random = new Random();
        String[] heros = {&quot;易大师&quot;,&quot;德邦&quot;,&quot;剑姬&quot;,&quot;盖伦&quot;,&quot;阿卡丽&quot;,&quot;金克斯&quot;,&quot;提莫&quot;,&quot;猴子&quot;,&quot;亚索&quot;};
        while(true){

            int index = random.nextInt(heros.length);
            //选择一个英雄
            String hero = heros[index];

            //开始玩游戏
            Thread.sleep(1000);

            //给集合中的该英雄的出场次数加1
            //第一次添加的时候，集合不存在，zincrby方法会创建
            jedis.zincrby(&quot;hero:ccl:phb&quot;, 1, hero);

            System.out.println(hero+&quot; 出场了.......&quot;);
        }
    }
}
</code></pre></li>
<li><p>LolBoxViewer</p>
<pre><code>/**
 * 英雄出场率排行榜查看模块
 * 
 * @author
 * 
 */
public class LolBoxViewer {
    public static void main(String[] args) throws Exception {
        Jedis jedis = new Jedis(&quot;192.168.59.101&quot;, 6379);
        int i = 1;
        while (true) {
            // 每隔3秒查看一次榜单
            Thread.sleep(3000);

            System.out.println(&quot;第&quot; + i + &quot;次查看榜单-----------&quot;);

            // 从redis中查询榜单的前N名
            Set&lt;Tuple&gt; topHeros = jedis.zrevrangeWithScores(&quot;hero:ccl:phb&quot;, 0, 4);

            for (Tuple t : topHeros) {

                System.out.println(t.getElement() + &quot;   &quot; + t.getScore());
            }
            i++;
            System.out.println(&quot;&quot;);
            System.out.println(&quot;&quot;);
            System.out.println(&quot;&quot;);
        }
    }
}
</code></pre></li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Redis应用-购物车推荐]]></title>
      <url>http://spark8.tech/2016/03/12/redisgouwuche/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h1><p>根据历史用户的购物车数据，对新用户的购物车做一个推荐</p>
<h1 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h1><ol>
<li>每来一个用户创建购物车，就对购物车中的每一件商品在redis中插入一条以商品名称为key的sortedset，成员为同时出现在购物车中的其他商品，分数为1</li>
<li>每新产生一个购物车，就对车中的商品更新redis中的sortedset，将同时出现的商品的分数增1</li>
<li><p>推荐时，用户放入一件商品到购物车，则从这件商品对应的sortedset中查询分数最高的同件商品，推荐给该用户<br><img src="/images/gouwuche.png" alt="redis实现购物车" title="redis实现购物车"></p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>核心代码RedisServer如下，完整项目请参见<a href="github.com/sparkliwei">github链接</a></p>
<pre><code>public class RedisService {

Jedis jedis = new Jedis(GlobalInstants.REDIS_SERVER, 6379);

/**
 * 将用户选择的商品添加到购物车 同时，将购物车信息添加到推荐池
 *
 * @param userName
 * @param newItem
 */
public void addToCart(String userName, String newItem, int incr) {

    //如果该用户的购物车中已经存在该商品，那么就不需要做推荐池数据的生成或更新
    if (!jedis.hexists(GlobalInstants.CART_PREFIX + userName, newItem)) {
        // 首先从该用户的购物车中将已经存在的商品取出来
        Set&lt;String&gt; oldItems = getCartItems(userName);   // oldItems : a b
        oldItems.remove(newItem);

        for (String oldItem : oldItems) {

            //为新商品创建一个推荐池，将每一个老商品添加进去
            jedis.zincrby(GlobalInstants.ITEM_RECOMMEND_POOL + newItem, 1, oldItem);
            //为每一个老商品的推荐池添加该新商品
            jedis.zincrby(GlobalInstants.ITEM_RECOMMEND_POOL + oldItem, 1, newItem);

        }

    }
    // 往购物车中添加商品及数量，购物车存放的数据结构为hash
    jedis.hincrBy(GlobalInstants.CART_PREFIX + userName, newItem, incr);

}

/**
 * 从redis中取出用户购物车中的商品及数量
 *
 * @param userName
 * @return
 */
public Map&lt;String, String&gt; getCart(String userName) {

    Map&lt;String, String&gt; itemAndNumb = jedis.hgetAll(GlobalInstants.CART_PREFIX + userName);

    return itemAndNumb;
}

/**
 * 查询购物车中的商品项
 *
 * @param userName
 * @return
 */
public Set&lt;String&gt; getCartItems(String userName) {

    Set&lt;String&gt; items = jedis.hkeys(GlobalInstants.CART_PREFIX + userName);

    return items;
}

/**
 * 根据给定的用户根据其购物车中的商品项推荐其他商品
 *
 * @param userName
 * @return
 */
public Map&lt;String, Double&gt; recommend(String userName) {

    //先拿到该用户当前购物车中所有的商品项
    Set&lt;String&gt; cartItems = getCartItems(userName);
    for (String item : cartItems) {
        // 将每一个商品项的推荐项添加到redis中的总推荐表
        // 合并sortedset的过程中，redis会自动将相同商品的推荐得分累加
        jedis.zunionstore(GlobalInstants.USER_RECOMMEND_POOL + userName, GlobalInstants.ITEM_RECOMMEND_POOL + item);

    }

    //从总推荐池去除用户购物车中已有的商品项
    jedis.zrem(GlobalInstants.USER_RECOMMEND_POOL + userName, cartItems.toString());

    //取出redis中的用户总推荐表中的前4名推荐商品
    Set&lt;Tuple&gt; recommends = jedis.zrevrangeWithScores(GlobalInstants.USER_RECOMMEND_POOL + userName, 0, 3);

    //将从redis中获取到的前4名推荐商品的名称及推荐得分封装到hashmap中返回给action
    HashMap&lt;String, Double&gt; recommendsMap = new HashMap&lt;String, Double&gt;();
    for (Tuple t : recommends) {
        recommendsMap.put(t.getElement(), t.getScore());
    }
    return recommendsMap;
}
}
</code></pre></li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Zookeeper-服务器上下线动态感知]]></title>
      <url>http://spark8.tech/2016/03/11/zkfbs/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<p>Zookeeper是一个分布式协调服务<br>在底层其实只提供了两个功能：<br><em>a、管理(存储，读取)用户程序提交的数据；PERSISTANT持久节点</em><br><em>b、并为用户程序提供数据节点监听服务； Ephemeral节点</em></p>
<h2 id="一、需求描述"><a href="#一、需求描述" class="headerlink" title="一、需求描述"></a>一、需求描述</h2><p>某分布式系统中，主节点可以有多台，可以动态上下线<br>任意一台客户端都能实时感知到主节点服务器的上下线</p>
<h2 id="二、设计思路"><a href="#二、设计思路" class="headerlink" title="二、设计思路"></a>二、设计思路</h2><p><img src="/images/zkfbs.png" alt="分布式共享锁" title="分布式共享锁"></p>
<h2 id="三、代码开发"><a href="#三、代码开发" class="headerlink" title="三、代码开发"></a>三、代码开发</h2><h3 id="1-客户端实现"><a href="#1-客户端实现" class="headerlink" title="1. 客户端实现"></a>1. 客户端实现</h3><pre><code>public class DistributedClient {
    private static final String connectString = &quot;mini1:2181,mini2:2181,mini3:2181&quot;;
    private static final int sessionTimeout = 2000;
    private static final String parentNode = &quot;/servers&quot;;
    // 注意:加volatile的意义何在？先写再读，保证读的结果一致
    private volatile List&lt;String&gt; serverList;
    private ZooKeeper zk = null;

    /**
     * 创建到zk的客户端连接
     * 
     * @throws Exception
     */
    public void getConnect() throws Exception {
        zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() {
            public void process(WatchedEvent event) {
            // 收到事件通知后的回调函数（应该是我们自己的事件处理逻辑）
                try {
                    //重新更新服务器列表，并且注册了监听
                    getServerList();
                } catch (Exception e) {
                }
            }
            });
    }

/**
 * 获取服务器信息列表
 * 
 * @throws Exception
     */
    public void getServerList() throws Exception {
        // 获取服务器子节点信息，并且对父节点进行监听
        List&lt;String&gt; children = zk.getChildren(parentNode, true);

        // 先创建一个局部的list来存服务器信息
        List&lt;String&gt; servers = new ArrayList&lt;String&gt;();
        for (String child : children) {
            // child只是子节点的节点名
            byte[] data = zk.getData(parentNode + &quot;/&quot; + child, false, null);
            servers.add(new String(data));
        }
        // 把servers赋值给成员变量serverList，已提供给各业务线程使用
        serverList = servers;

        //打印服务器列表
        System.out.println(serverList);
    }

    /**
     * 业务功能
     * @throws InterruptedException
     */
    public void handleBussiness() throws InterruptedException {
        System.out.println(&quot;client start working.....&quot;);
        Thread.sleep(Long.MAX_VALUE);
    }

    public static void main(String[] args) throws Exception {
        // 获取zk连接
        DistributedClient client = new DistributedClient();
        client.getConnect();
        // 获取servers的子节点信息（并监听），从中获取服务器信息列表
        client.getServerList();
        // 业务线程启动
        client.handleBussiness();
    }
 }
</code></pre><h3 id="2-服务端实现"><a href="#2-服务端实现" class="headerlink" title="2. 服务端实现"></a>2. 服务端实现</h3><pre><code>public class DistributedServer {

  private static final String CONNECT_STRING   = &quot;mini1:2181,mini2:2181,mini3:2181&quot;;
  private static final int    SESSION_TIME_OUT = 2000;
  private static final String PARENT_NODE      = &quot;/servers&quot;;

  private ZooKeeper zk = null;

  /**
   * 创建到zk的客户端连接
   *
   * @throws Exception
   */
  public void getConnect() throws Exception {

      zk = new ZooKeeper(CONNECT_STRING, SESSION_TIME_OUT, new Watcher() {
          public void process(WatchedEvent event) {
              // 收到事件通知后的回调函数（应该是我们自己的事件处理逻辑）
              System.out.println(event.getType() + &quot;---&quot; + event.getPath());
              /*try {
        zk.getChildren(&quot;/&quot;, true);
    } catch (Exception e) {
    }*/
          }
      });
  }

  /**
   * 向zk集群注册服务器信息
   * @param hostname
   * @throws Exception
   */
  public void registerServer(String hostname) throws Exception {
      Stat exists = zk.exists(PARENT_NODE, false);
      if (exists == null) zk.create(PARENT_NODE, null, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
      String path = zk.create(PARENT_NODE + &quot;/server&quot;, hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
      System.out.println(hostname + &quot;is online..&quot; + path);
  }

  /**
   * 业务功能
   * @throws InterruptedException
   */
  public void handleBussiness(String hostname) throws InterruptedException {
      System.out.println(hostname + &quot;start working.....&quot;);
      Thread.sleep(Long.MAX_VALUE);
  }

  public static void main(String[] args) throws Exception {

      DistributedServer server = new DistributedServer();
      // 获取zk连接
      server.getConnect();

      // 利用zk连接注册服务器信息(主机名)
      server.registerServer(args[0]);

      // 启动业务功能
      server.handleBussiness(args[0]);
  }
</code></pre><p>  }</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Zookeeper-分布式共享锁]]></title>
      <url>http://spark8.tech/2016/03/11/zkfbss/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="一、需求描述"><a href="#一、需求描述" class="headerlink" title="一、需求描述"></a>一、需求描述</h2><p>  在我们自己的分布式业务系统中，可能会存在某种资源，需要被整个系统的各台服务器共享访问，但是只允许一台服务器同时访问</p>
<h2 id="二、设计思路"><a href="#二、设计思路" class="headerlink" title="二、设计思路"></a>二、设计思路</h2><ol>
<li>连接zookeeper，process监听子节点变化事件，接收事件按规则写业务逻辑，并重新注册锁</li>
<li>向zookeeper注册一把锁</li>
<li>从zk的锁父目录下，获取所有子节点，并且注册对父节点的监听</li>
<li>如果争抢资源的程序就只有自己，则可以直接去访问共享资源</li>
<li><p>访问资源后删除锁</p>
<h2 id="三、代码开发"><a href="#三、代码开发" class="headerlink" title="三、代码开发"></a>三、代码开发</h2><p> public class DistributedClientLock {</p>
</li>
</ol>
<pre><code>// 会话超时
private static final int SESSION_TIMEOUT = 2000;
// zookeeper集群地址
private String hosts = &quot;mini1:2181,mini2:2181,mini3:2181&quot;;
private String groupNode = &quot;locks&quot;;
private String subNode = &quot;sub&quot;;
private boolean haveLock = false;

private ZooKeeper zk;
// 记录自己创建的子节点路径
private volatile String thisPath;

/**
 * 连接zookeeper
 */
public void connectZookeeper() throws Exception {
    zk = new ZooKeeper(hosts, SESSION_TIMEOUT, new Watcher() {
        public void process(WatchedEvent event) {
            try {

                // 判断事件类型，此处只处理子节点变化事件
                if (event.getType() == EventType.NodeChildrenChanged &amp;&amp; event.getPath().equals(&quot;/&quot; + groupNode)) {
                    //获取子节点，并对父节点进行监听
                    List&lt;String&gt; childrenNodes = zk.getChildren(&quot;/&quot; + groupNode, true);
                    String thisNode = thisPath.substring((&quot;/&quot; + groupNode + &quot;/&quot;).length());
                    // 去比较是否自己是最小id
                    Collections.sort(childrenNodes);
                    if (childrenNodes.indexOf(thisNode) == 0) {
                        //访问共享资源处理业务，并且在处理完成之后删除锁
                        doSomething();

                        //重新注册一把新的锁
                        thisPath = zk.create(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, null, Ids.OPEN_ACL_UNSAFE,
                                CreateMode.EPHEMERAL_SEQUENTIAL);
                    }
                }
        } catch (Exception e) {
                e.printStackTrace();
            }
        }
    });

    // 1、程序一进来就先注册一把锁到zk上
    thisPath = zk.create(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, null, Ids.OPEN_ACL_UNSAFE,
            CreateMode.EPHEMERAL_SEQUENTIAL);

    // wait一小会，便于观察
    Thread.sleep(new Random().nextInt(1000));

    // 从zk的锁父目录下，获取所有子节点，并且注册对父节点的监听
    List&lt;String&gt; childrenNodes = zk.getChildren(&quot;/&quot; + groupNode, true);

    // 如果争抢资源的程序就只有自己，则可以直接去访问共享资源 
    if (childrenNodes.size() == 1) {
        doSomething();
        thisPath = zk.create(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, null, Ids.OPEN_ACL_UNSAFE,
                CreateMode.EPHEMERAL_SEQUENTIAL);
    }
}

/**
 * 处理业务逻辑，并且在最后释放锁
 */
private void doSomething() throws Exception {
    try {
        System.out.println(&quot;gain lock: &quot; + thisPath);
        Thread.sleep(2000);
        // do something
    } finally {
        System.out.println(&quot;finished: &quot; + thisPath);
//删除之前的锁节点
        zk.delete(this.thisPath, -1);
    }
}

public static void main(String[] args) throws Exception {
    DistributedClientLock dl = new DistributedClientLock();
    dl.connectZookeeper();
    Thread.sleep(Long.MAX_VALUE);
}
</code></pre><p>}</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Zookeeper-原理补充]]></title>
      <url>http://spark8.tech/2016/03/11/zkyuanli/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="Zookepper的选举机制（保证数据一致性的核心算法paxos）"><a href="#Zookepper的选举机制（保证数据一致性的核心算法paxos）" class="headerlink" title="Zookepper的选举机制（保证数据一致性的核心算法paxos）"></a>Zookepper的选举机制（保证数据一致性的核心算法paxos）</h2><p>以一个简单的例子来说明整个选举的过程.<br>假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么.<br>1) 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态<br>2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态.<br>3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader.<br>4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了.<br>5) 服务器5启动,同4一样,当小弟.</p>
<h2 id="非全新集群的选举机制-数据恢复"><a href="#非全新集群的选举机制-数据恢复" class="headerlink" title="非全新集群的选举机制(数据恢复)"></a>非全新集群的选举机制(数据恢复)</h2><p>那么，初始化的时候，是按照上述的说明进行选举的，但是当zookeeper运行了一段时间之后，有机器down掉，重新选举时，选举过程就相对复杂了。<br>需要加入数据version、leader id和逻辑时钟。<br>数据version：数据新的version就大，数据每次更新都会更新version。<br>Leader id：就是我们配置的myid中的值，每个机器一个。<br>逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说:  如果在同一次选举中,那么这个值应该是一致的 ;  逻辑时钟值越大,说明这一次选举leader的进程更新.<br>选举的标准就变成：<br>1、逻辑时钟小的选举结果被忽略，重新投票<br>2、统一逻辑时钟后，数据id大的胜出<br>3、数据id相同的情况下，leader id大的胜出<br>根据这个规则选出leader。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop2.x——关于HDFS一些有意思的总结]]></title>
      <url>http://spark8.tech/2015/11/11/hdfs/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="1-HDFS-文件存放"><a href="#1-HDFS-文件存放" class="headerlink" title="1. HDFS 文件存放"></a>1. HDFS 文件存放</h2><p>namenode   管理元数据（文件路径、副本数、文件的blockid，位置等信息）<br>datanode   用户的文件存放在datanode上，放在配置的目录dfs.datanode.data.dir下<br>Q：namenode的fsimage和edits.xml文件中有没有文件路径呢？<br>A：系统中数据块的位置不是由namenode维护的，里面只存blockid，name node启动时加载fsimage和edits的各项操作，namenode启动时首先运行于安全模式，即对于客户端是只读的。安全模式下namenode不会向datanode发出任何块复制或删除指令，这时是没有必要的，namenode只需等待更多的datanode发送块列表信息即可，当接收到足够多的块位置信息，满足“最小复本条件”后，namenode会在30秒后退出安全模式。<br> hdfs  dfsadmin -report命令可以查看hdfs集群的状态</p>
<blockquote>
<p>参见链接<a href="http://www.cnblogs.com/zhaosk/p/4375530.html" target="_blank" rel="external">http://www.cnblogs.com/zhaosk/p/4375530.html</a></p>
<h2 id="2-HDFS文件副本"><a href="#2-HDFS文件副本" class="headerlink" title="2. HDFS文件副本"></a>2. HDFS文件副本</h2><p>Q：hdfs上的文件可以随时更改副本数吗？hdfs上的文件可以具有不同的副本数吗？<br>A:  可以的,我们一般是通过客户端连接hdfs（本质上FileSyste.get(conf)和hadoop fs都属于客户端），客户端设定replication conf  > local jar > hdfs default。</p>
<h2 id="3-HDFS读写数据流程"><a href="#3-HDFS读写数据流程" class="headerlink" title="3. HDFS读写数据流程"></a>3. HDFS读写数据流程</h2><p>Q： hdfs适合做网盘么？<br>A：<br>不合适。HDFS在小文件场景下效率较低，并且文件读写速度太慢。下面是hdfs的读写数据流程？<br>hdfs读流程：<br>1、客户端向namenode请求读取文件（指定一个路径）<br>2、namenode查询元数据信息，看文件是否存在，看文件的block信息，并返回给客户端<br>3、客户端拿到block信息后，从第一个block开始，找到一台最近的datanode<br>4、客户端跟这台datanode发出读数据请求，建立文件传输通道，开始接收数据到客户端本地<br>5、接收完一个block后，再重复以上过程找到另一个datanode接收下一个block<br><img src="/images/hdfsdu.png" alt="" title="hdfsdu"><br>hdfs写流程：<br>1、客户端向namenode请求上传文件（指定一个路径）<br>2、namenode查询元数据信息，看路径是否存在<br>3、客户端就请求上传第一个block，指定副本数量、blocksize等参数<br>4、namenode收到请求后，找到若干台（副本数量）datanode信息返回给客户端<br>5、客户端跟第一个datanode发出请求建立数据传输通道，第一个datanode会继续向第2个dn发出通道建立请求，直到pipeline建立成功<br>6、pipeline成功后，客户端就开始向第1个datanode传输第一个block的数据，第1个就向第2个，第2个就向第3个传输数据<br>7、待第一个block上传成功后，继续向namenode请求上传第二个block块<br><img src="/images/hdfsxie.png" alt="" title="hdfsxie"></p>
<h2 id="4-动态增加datanode"><a href="#4-动态增加datanode" class="headerlink" title="4. 动态增加datanode"></a>4. 动态增加datanode</h2><p>Q：如何动态增加一台datanode？<br>A： 超乎想象的easy，准备一台服务器，装好环境，解压一个hadoop安装包，然后从其他机器拷贝配置文件过来覆盖，网络连入集群即可。<br>Q： 怎么会如此简单？不是和配置的etc/hadoop/slave文件有关系么？<br>A： slave文件是为了让我们可以利用start-dfs.sh对集群进行批量启动而已，只要我们设置了namenode，datanode会与namenode通信定时汇报最新的块列表信息，namenode即将datanode加入集群中。<br>参考链接：<a href="http://www.cnblogs.com/imzye/p/5174169.html" target="_blank" rel="external">http://www.cnblogs.com/imzye/p/5174169.html</a></p>
<h2 id="5-secondary-namenode是什么"><a href="#5-secondary-namenode是什么" class="headerlink" title="5. secondary namenode是什么"></a>5. secondary namenode是什么</h2><p>Q： secondary namenode是否是namenode的备份节点？<br>A： 不是的，SecondaryNameNode定期地合并fsimage和Edits日志文件，并保持Edits日志文件的大小在一个上限值内，由于它的内存需求与NameNode的一致，所以它通常运行在NameNode以外的一台机器上。Secondary NameNode以NameNode目录结构的相同方式存储合并后的命名空间镜像的副本，以备在namenode发生故障时使用。<br><img src="/images/hdfssecondary.png" alt="" title="hdfssecondary"></p>
<h2 id="6-提高namenode元数据的安全性"><a href="#6-提高namenode元数据的安全性" class="headerlink" title="6. 提高namenode元数据的安全性"></a>6. 提高namenode元数据的安全性</h2><p>在未做HA的前提下…<br>Q:  如果namenode的元数据损坏，如何恢复？<br>A：<br>1、拷贝secondary namenode节点上工作目录中的namesecondary文件夹到namenode所在节点的工作目录中<br>2、将namesecondary重名为name<br>3、用hadoop-daemon.sh start namenode 启动namenode即可<br>但是由于namenode和secondarynamenode合并需要触发容量阈值和时间阈值，有可能中途宕机丢失数据，所以必须采取更多措施来保证元数据的安全性。<br>Q： 如何尽可能提高安全性呢？<br>A： 将namenode的元数据目录配置到多块磁盘上，或者配置到NFS文件系统中<br>先在linux系统中挂载磁盘<br>  sda1  —> mount /dev/sda1  /metadata/disk1<br>  sdb1  —> mount /dev/sdb1  /metadata/disk2<br>  sdc1  —> mount /dev/sdc1  /metadata/disk3<br>然后配置hadoop<br>     <property><br>     <name>dfs.namenode.name.dir</name><br>     <value>/metadata/disk1,/metadata/disk2,/metadata/disk3</value><br>   </property></p>
</blockquote>
<p>namenode就会将元数据完整地同时写入三块磁盘</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Flume——自定义拦截器(脱敏处理)]]></title>
      <url>http://spark8.tech/2015/10/28/flumetuomin/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>有源数据<br><img src="/images/flumewtm.png" alt="" title="flumewtm"><br>要求进行脱敏处理，第1列手机号加密传输，再取2、4、6、7列数据</p>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>自定义Flume拦截器，过滤掉不需要的字段，并对指定字段加密处理，将源数据进行预处理。减少数据传输量，降低存储开销。<br>技术方案核心包括二部分：</p>
<ol>
<li>编写java代码，自定义拦截器；</li>
<li>修改Flume的配置信息<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h3></li>
<li>定义一个类CustomParameterInterceptor实现Interceptor接口。</li>
<li>在CustomParameterInterceptor类中定义变量，这些变量是需要到 Flume的配置文件中进行配置使用的。每一行字段间的分隔符(fields_separator)、通过分隔符分隔后，所需要列字段的下标（indexs）、多个下标使用的分隔符（indexs_separator)、多个下标使用的分隔符（indexs_separator)。</li>
<li>添加CustomParameterInterceptor的有参构造方法。并对相应的变量进行处理。将配置文件中传过来的unicode编码进行转换为字符串。</li>
<li>写具体的要处理的逻辑intercept()方法，一个是单个处理的，一个是批量处理。</li>
<li>接口中定义了一个内部接口Builder，在configure方法中，进行一些参数配置。并给出，在flume的conf中没配置一些参数时，给出其默认值。通过其builder方法，返回一个CustomParameterInterceptor对象。</li>
<li><p>定义一个静态类，类中封装MD5加密方法<br>通过以上步骤，自定义拦截器的代码开发已完成，然后打包成jar， 放到Flume的根目录下的lib中</p>
<pre><code>package cn.holley.interceptor;

import static cn.holley.interceptor.CustomParameterInterceptor.Constants.FIELD_SEPARATOR;
import static cn.holley.interceptor.CustomParameterInterceptor.Constants.DEFAULT_FIELD_SEPARATOR;
import static cn.holley.interceptor.CustomParameterInterceptor.Constants.INDEXS;
import static cn.holley.interceptor.CustomParameterInterceptor.Constants.DEFAULT_INDEXS;
import static cn.holley.interceptor.CustomParameterInterceptor.Constants.INDEXS_SEPARATOR;
import static cn.holley.interceptor.CustomParameterInterceptor.Constants.DEFAULT_INDEXS_SEPARATOR;
import static cn.holley.interceptor.CustomParameterInterceptor.Constants.ENCRYPTED_FIELD_INDEX;
import static cn.holley.interceptor.CustomParameterInterceptor.Constants.DEFAULT_ENCRYPTED_FIELD_INDEX;

import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.util.ArrayList;
import java.util.List;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;
import org.junit.Test;

import com.google.common.base.Charsets;

/**
 *
 */
public class CustomParameterInterceptor  implements Interceptor{    

    /** The field_separator.指明每一行字段的分隔符 */
    private final String fields_separator;

    /** The indexs.通过分隔符分割后，指明需要那列的字段 下标*/
    private final String indexs;

    /** The indexs_separator. 多个下标的分隔符*/
    private final String indexs_separator;

/**
 * 
 * @param regex
 * @param field_separator
 * @param indexs
 * @param indexs_separator
 */
public CustomParameterInterceptor( String fields_separator,
        String indexs, String indexs_separator,String encrypted_field_index) {
    String f = fields_separator.trim();
    String i = indexs_separator.trim();
    this.indexs = indexs;
    this.encrypted_field_index=encrypted_field_index.trim();
    if (!f.equals(&quot;&quot;)) {
        f = UnicodeToString(f);
    }
    this.fields_separator =f;
    if (!i.equals(&quot;&quot;)) {
        i = UnicodeToString(i);
    }
    this.indexs_separator = i;
}

/*
 * 
 * \t 制表符 (&apos;\u0009&apos;) \n 新行（换行）符 (&apos; &apos;) \r 回车符 (&apos; &apos;) \f 换页符 (&apos;\u000C&apos;) \a 报警
 * (bell) 符 (&apos;\u0007&apos;) \e 转义符 (&apos;\u001B&apos;) \cx  空格(\u0020)对应于 x 的控制符
 * 
 * @param str
 * @return
 * @data:2015-6-30
 */

/** The encrypted_field_index. 需要加密的字段下标*/
private final String encrypted_field_index;
</code></pre></li>
</ol>
<pre><code>public static String UnicodeToString(String str) {
    Pattern pattern = Pattern.compile(&quot;(\\\\u(\\p{XDigit}{4}))&quot;);
    Matcher matcher = pattern.matcher(str);
    char ch;
    while (matcher.find()) {
        ch = (char) Integer.parseInt(matcher.group(2), 16);
        str = str.replace(matcher.group(1), ch + &quot;&quot;);
    }
    return str;
}

/*
 * @see org.apache.flume.interceptor.Interceptor#intercept(org.apache.flume.Event)
 * 单个event拦截逻辑
 */
@Override
public Event intercept(Event event) {
    if (event == null) {
        return null;
    }        
    try {
        String line = new String(event.getBody(), Charsets.UTF_8);
        String[] fields_spilts = line.split(fields_separator);
        String[] indexs_split = indexs.split(indexs_separator);
        String newLine=&quot;&quot;;
        for (int i = 0; i &lt; indexs_split.length; i++) {
            int parseInt = Integer.parseInt(indexs_split[i]);    
            //对加密字段进行加密
            if(!&quot;&quot;.equals(encrypted_field_index)&amp;&amp;encrypted_field_index.equals(indexs_split[i])){
                newLine+=StringUtils.GetMD5Code(fields_spilts[parseInt]);                
            }else{                    
                newLine+=fields_spilts[parseInt];
            }

            if(i!=indexs_split.length-1){
                newLine+=fields_separator;
            }
        }            
        event.setBody(newLine.getBytes(Charsets.UTF_8));
        return event;
    } catch (Exception e) {
        return event;
    }        
}

/* 
 * @see org.apache.flume.interceptor.Interceptor#intercept(java.util.List)
 * 批量event拦截逻辑
 */
@Override
public List&lt;Event&gt; intercept(List&lt;Event&gt; events) {
    List&lt;Event&gt; out = new ArrayList&lt;Event&gt;();
    for (Event event : events) {
        Event outEvent = intercept(event);
        if (outEvent != null) {
            out.add(outEvent);
        }
    }
    return out;
}

/* 
 * @see org.apache.flume.interceptor.Interceptor#initialize()
 */
@Override
public void initialize() {
    // TODO Auto-generated method stub

}

/*
 * @see org.apache.flume.interceptor.Interceptor#close()
 */
@Override
public void close() {
    // TODO Auto-generated method stub

}


/**
 * 相当于自定义Interceptor的工厂类
 * 在flume采集配置文件中通过制定该Builder来创建Interceptor对象
 * 可以在Builder中获取、解析flume采集配置文件中的拦截器Interceptor的自定义参数：
 * 字段分隔符，字段下标，下标分隔符、加密字段下标 ...等
 * @author
 *
 */
public static class Builder implements Interceptor.Builder {

    /** The fields_separator.指明每一行字段的分隔符 */
    private  String fields_separator;

    /** The indexs.通过分隔符分割后，指明需要那列的字段 下标*/
    private  String indexs;

    /** The indexs_separator. 多个下标下标的分隔符*/
    private  String indexs_separator;

    /** The encrypted_field. 需要加密的字段下标*/
    private  String encrypted_field_index; 

    /* 
     * @see org.apache.flume.conf.Configurable#configure(org.apache.flume.Context)
     */
    @Override
    public void configure(Context context) {
        fields_separator = context.getString(FIELD_SEPARATOR, DEFAULT_FIELD_SEPARATOR);
        indexs = context.getString(INDEXS, DEFAULT_INDEXS);
        indexs_separator = context.getString(INDEXS_SEPARATOR, DEFAULT_INDEXS_SEPARATOR);
        encrypted_field_index= context.getString(ENCRYPTED_FIELD_INDEX, DEFAULT_ENCRYPTED_FIELD_INDEX);

    }

    /* 
     * @see org.apache.flume.interceptor.Interceptor.Builder#build()
     */
    @Override
    public Interceptor build() {

        return new CustomParameterInterceptor(fields_separator, indexs, indexs_separator,encrypted_field_index);
    }
}    


/**
 * 常量
 *
 */
public static class Constants {
    /** The Constant FIELD_SEPARATOR. */
    public static final String FIELD_SEPARATOR = &quot;fields_separator&quot;;

    /** The Constant DEFAULT_FIELD_SEPARATOR. */
    public static final String DEFAULT_FIELD_SEPARATOR =&quot; &quot;;

    /** The Constant INDEXS. */
    public static final String INDEXS = &quot;indexs&quot;;

    /** The Constant DEFAULT_INDEXS. */
    public static final String DEFAULT_INDEXS = &quot;0&quot;;

    /** The Constant INDEXS_SEPARATOR. */
    public static final String INDEXS_SEPARATOR = &quot;indexs_separator&quot;;

    /** The Constant DEFAULT_INDEXS_SEPARATOR. */
    public static final String DEFAULT_INDEXS_SEPARATOR = &quot;,&quot;;

    /** The Constant ENCRYPTED_FIELD_INDEX. */
    public static final String ENCRYPTED_FIELD_INDEX = &quot;encrypted_field_index&quot;;

    /** The Constant DEFAUL_TENCRYPTED_FIELD_INDEX. */
    public static final String DEFAULT_ENCRYPTED_FIELD_INDEX = &quot;&quot;;

    /** The Constant PROCESSTIME. */
    public static final String PROCESSTIME = &quot;processTime&quot;;
    /** The Constant PROCESSTIME. */
    public static final String DEFAULT_PROCESSTIME = &quot;a&quot;;

}



/**
 * 工具类：字符串md5加密
 */
public static class StringUtils {
       // 全局数组
    private final static String[] strDigits = { &quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;,
            &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot; };

    // 返回形式为数字跟字符串
    private static String byteToArrayString(byte bByte) {
        int iRet = bByte;
        // System.out.println(&quot;iRet=&quot;+iRet);
        if (iRet &lt; 0) {
            iRet += 256;
        }
        int iD1 = iRet / 16;
        int iD2 = iRet % 16;
        return strDigits[iD1] + strDigits[iD2];
    }

    // 返回形式只为数字
    private static String byteToNum(byte bByte) {
        int iRet = bByte;
        System.out.println(&quot;iRet1=&quot; + iRet);
        if (iRet &lt; 0) {
            iRet += 256;
        }
        return String.valueOf(iRet);
    }

    // 转换字节数组为16进制字串
    private static String byteToString(byte[] bByte) {
        StringBuffer sBuffer = new StringBuffer();
        for (int i = 0; i &lt; bByte.length; i++) {
            sBuffer.append(byteToArrayString(bByte[i]));
        }
        return sBuffer.toString();
    }

    public static String GetMD5Code(String strObj) {
        String resultString = null;
        try {
            resultString = new String(strObj);
            MessageDigest md = MessageDigest.getInstance(&quot;MD5&quot;);
            // md.digest() 该函数返回值为存放哈希值结果的byte数组
            resultString = byteToString(md.digest(strObj.getBytes()));
        } catch (NoSuchAlgorithmException ex) {
            ex.printStackTrace();
        }
        return resultString;
    }
    }

}
</code></pre><h3 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h3><p>修改Flume的配置信息<br>        a1.channels = c1<br>        a1.sources = r1<br>        a1.sinks = s1</p>
<pre><code>channel
a1.channels.c1.type = memory
a1.channels.c1.capacity=100000
a1.channels.c1.transactionCapacity=50000

source
a1.sources.r1.channels = c1
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /home/hadoop/logs/
a1.sources.r1.batchSize= 50
a1.sources.r1.inputCharset = UTF-8

a1.sources.r1.interceptors =i1 i2
a1.sources.r1.interceptors.i1.type =cn.holley.interceptor.CustomParameterInterceptor$Builder
a1.sources.r1.interceptors.i1.fields_separator=\\u0009
a1.sources.r1.interceptors.i1.indexs =0,1,3,5,6
a1.sources.r1.interceptors.i1.indexs_separator =\\u002c
a1.sources.r1.interceptors.i1.encrypted_field_index =0

a1.sources.r1.interceptors.i2.type = org.apache.flume.interceptor.TimestampInterceptor$Builder


sink
a1.sinks.s1.channel = c1
a1.sinks.s1.type = hdfs
a1.sinks.s1.hdfs.path =hdfs://hdp-node-01:9000/flume/%Y%m%d
a1.sinks.s1.hdfs.filePrefix = event
a1.sinks.s1.hdfs.fileSuffix = .log
a1.sinks.s1.hdfs.rollSize = 10485760
a1.sinks.s1.hdfs.rollInterval =20
a1.sinks.s1.hdfs.rollCount = 0
a1.sinks.s1.hdfs.batchSize = 1500
a1.sinks.s1.hdfs.round = true
a1.sinks.s1.hdfs.roundUnit = minute
a1.sinks.s1.hdfs.threadsPoolSize = 25
a1.sinks.s1.hdfs.useLocalTimeStamp = true
a1.sinks.s1.hdfs.minBlockReplicas = 1
a1.sinks.s1.hdfs.fileType =DataStream
a1.sinks.s1.hdfs.writeFormat = Text
a1.sinks.s1.hdfs.callTimeout = 60000
a1.sinks.s1.hdfs.idleTimeout =60
</code></pre><h2 id="实现效果"><a href="#实现效果" class="headerlink" title="实现效果"></a>实现效果</h2><p><img src="/images/result1.png" alt="" title="result1"><br><img src="/images/result2.png" alt="" title="result2"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Flume——日志采集与汇总]]></title>
      <url>http://spark8.tech/2015/10/24/flumerizhi/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>A、B两台日志服务机器实时生产日志主要类型为access.log、nginx.log、web.log<br>现在要求：<br>把A、B 机器中的access.log、nginx.log、web.log 采集汇总到C机器上然后统一收集到hdfs中。<br>但是在hdfs中要求的目录为：<br>/source/logs/access/20160101/<strong> /source/logs/nginx/20160101/</strong> /source/logs/web/20160101/** ## 设计思路<br>难点在于区分日志服务器发送过来的不同文件夹，使用拦截器<br><img src="/images/flume1.png" alt="" title="flume"><br>flume架构如下图所示<br><img src="/images/flume2.png" alt="" title="flume"></p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>flume主要是通过配置实现</p>
<p>服务器A对应的IP为 192.168.200.102<br>服务器B对应的IP为 192.168.200.103<br>服务器C对应的IP为 192.168.200.101</p>
<ol>
<li><p>在服务器A和服务器B上的$FLUME_HOME/conf 创建配置文件    exec_source_avro_sink.conf  文件内容为</p>
<pre><code>Name the components on this agent
a1.sources = r1 r2 r3
a1.sinks = k1
a1.channels = c1

Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /root/data/access.log
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = static
 static拦截器的功能就是往采集到的数据的header中插入自己定## 义的key-value对
a1.sources.r1.interceptors.i1.key = type
a1.sources.r1.interceptors.i1.value = access

a1.sources.r2.type = exec
a1.sources.r2.command = tail -F /root/data/nginx.log
a1.sources.r2.interceptors = i2
a1.sources.r2.interceptors.i2.type = static
a1.sources.r2.interceptors.i2.key = type
a1.sources.r2.interceptors.i2.value = nginx

a1.sources.r3.type = exec
a1.sources.r3.command = tail -F /root/data/web.log
a1.sources.r3.interceptors = i3
a1.sources.r3.interceptors.i3.type = static
a1.sources.r3.interceptors.i3.key = type
a1.sources.r3.interceptors.i3.value = web

Describe the sink
a1.sinks.k1.type = avro
a1.sinks.k1.hostname = 192.168.200.101
a1.sinks.k1.port = 41414

Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 20000
a1.channels.c1.transactionCapacity = 10000

Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sources.r2.channels = c1
a1.sources.r3.channels = c1
a1.sinks.k1.channel = c1
</code></pre></li>
</ol>
<ol>
<li><p>在服务器C上的$FLUME_HOME/conf 创建配置文件    avro_source_hdfs_sink.conf  文件内容为</p>
<pre><code>定义agent名， source、channel、sink的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

定义source
a1.sources.r1.type = avro
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port =41414

添加时间拦截器
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = org.apache.flume.interceptor.TimestampInterceptor$Builder

定义channels
a1.channels.c1.type = memory
a1.channels.c1.capacity = 20000
a1.channels.c1.transactionCapacity = 10000

定义sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path=hdfs://192.168.200.101:9000/source/logs/%{type}/%Y%m%d
a1.sinks.k1.hdfs.filePrefix =events
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
时间类型
a1.sinks.k1.hdfs.useLocalTimeStamp = true
生成的文件不按条数生成
a1.sinks.k1.hdfs.rollCount = 0
生成的文件按时间生成
a1.sinks.k1.hdfs.rollInterval = 30
生成的文件按大小生成
a1.sinks.k1.hdfs.rollSize  = 10485760
批量写入hdfs的个数
a1.sinks.k1.hdfs.batchSize = 10000
flume操作hdfs的线程数（包括新建，写入等）
a1.sinks.k1.hdfs.threadsPoolSize=10
操作hdfs超时时间
a1.sinks.k1.hdfs.callTimeout=30000

组装source、channel、sink
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1    
</code></pre></li>
<li><p>配置完成之后，在服务器A和B上的/root/data有数据文件access.log、nginx.log、web.log。先启动服务器C上的flume，启动命令<br>在flume安装目录下执行 ：<br>  bin/flume-ng agent -c conf -f conf/avro_source_hdfs_sink.conf -name a1 -Dflume.root.logger=DEBUG,console    </p>
</li>
</ol>
<p>然后在启动服务器上的A和B，启动命令<br>在flume安装目录下执行 ：<br>     bin/flume-ng agent -c conf -f conf/exec_source_avro_sink.conf -name a1 -Dflume.root.logger=DEBUG,console    </p>
<h2 id="实现效果"><a href="#实现效果" class="headerlink" title="实现效果"></a>实现效果</h2><p><img src="/images/flume3.png" alt="" title="flume3"><br><img src="/images/flume4.png" alt="" title="flume4"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hive——transform]]></title>
      <url>http://spark8.tech/2015/10/20/transform/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>如果你不想写UDP函数，而实现的功能很容易用脚本实现，那么尝试下transform也是个不错的选择。<br>还是以上文中jsonParse的案例来说明。</p>
<h2 id="思路及实现"><a href="#思路及实现" class="headerlink" title="思路及实现"></a>思路及实现</h2><ol>
<li>先加载rating.json文件到hive的一个原始表 rat_json<pre><code>create table rat_json(line string) row format delimited;
load data local inpath &apos;/home/hadoop/rating.json&apos; into table rat_json;
</code></pre></li>
<li>需要解析json数据成四个字段，插入一张新的表 t_rating<pre><code>insert overwrite table t_rating
select get_json_object(line,&apos;$.movie&apos;) as moive,get_json_object(line,&apos;$.rate&apos;) as rate  from rat_json;
</code></pre></li>
<li><p>使用transform+python的方式去转换unixtime为weekday<br>先编辑一个python脚本文件</p>
<pre><code>vi weekday\_mapper.py
!/bin/python
import sys
import datetime

for line in sys.stdin:
  line = line.strip()
  movieid, rating, unixtime,userid = line.split(&apos;\t&apos;)
  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
  print &apos;\t&apos;.join([movieid, rating, str(weekday),userid])
</code></pre><p>保存文件<br>然后，将文件加入hive的classpath：</p>
<pre><code>hive&gt;add FILE /home/hadoop/weekday_mapper.py;
</code></pre><p>执行HQL</p>
<pre><code>hive&gt;create TABLE u_data_new as
SELECT
  TRANSFORM (movieid, rate, timestring,uid)
  USING &apos;python weekday_mapper.py&apos;
  AS (movieid, rate, weekday,uid)
FROM t_rating;
</code></pre><p>检查结果</p>
<pre><code>select distinct(weekday) from u_data_new limit 10;
</code></pre></li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hive ——内置函数与UDF实现]]></title>
      <url>http://spark8.tech/2015/10/19/udf/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>有原始json数据如下：<br><img src="/images/json1.png" alt="" title="json1"><br>需要将数据导入到hive数据仓库中<br><img src="/images/biao1.png" alt="" title="biao"></p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="内置函数get-json-object"><a href="#内置函数get-json-object" class="headerlink" title="内置函数get_json_object"></a>内置函数get_json_object</h3><pre><code>select get_json_object(line,&apos;$.movie&apos;) as moive,get_json_object(line,&apos;$.rate&apos;) as rate  from t_json limit 10;
</code></pre><h3 id="UDF实现"><a href="#UDF实现" class="headerlink" title="UDF实现"></a>UDF实现</h3><ol>
<li><p>先开发一个java类，继承UDF，并重载evaluate方法<br>JsonParser</p>
<pre><code>public class JsonParser extends UDF {

public String evaluate(String jsonLine) {

ObjectMapper objectMapper = new ObjectMapper();

try {
MovieRateBean bean = objectMapper.readValue(jsonLine, MovieRateBean.class);
return bean.toString();
} catch (Exception e) {

}
return &quot;&quot;;
}

//public static void main(String[] args) throws JsonParseException, JsonMappingException, IOException {
//    String str = &quot;{\&quot;movie\&quot;:\&quot;1193\&quot;,\&quot;rate\&quot;:\&quot;5\&quot;,\&quot;timeStamp\&quot;:\&quot;978300760\&quot;,\&quot;uid\&quot;:\&quot;1\&quot;}&quot;;
//    ObjectMapper objectMapper = new ObjectMapper();
//    MovieRateBean readValue = objectMapper.readValue(str, MovieRateBean.class);
//    System.out.println(readValue);
//}
}
</code></pre><p>MovieRateBean</p>
<pre><code>//{&quot;movie&quot;:&quot;1721&quot;,&quot;rate&quot;:&quot;3&quot;,&quot;timeStamp&quot;:&quot;965440048&quot;,&quot;uid&quot;:&quot;5114&quot;}
public class MovieRateBean {

private String movie;
private String rate;
private String timeStamp;
private String uid;
public String getMovie() {
return movie;
}
public void setMovie(String movie) {
this.movie = movie;
}
public String getRate() {
return rate;
}
public void setRate(String rate) {
this.rate = rate;
}
public String getTimeStamp() {
return timeStamp;
}
public void setTimeStamp(String timeStamp) {
this.timeStamp = timeStamp;
}
public String getUid() {
return uid;
}
public void setUid(String uid) {
this.uid = uid;
}
@Override
public String toString() {
return movie + &quot;\t&quot; + rate + &quot;\t&quot; + timeStamp + &quot;\t&quot; + uid;
}
}
</code></pre></li>
<li><p>打成jar包上传到服务器</p>
</li>
<li>将jar包添加到hive的classpath<pre><code>hive&gt;add JAR /home/hadoop/udf.jar;
</code></pre></li>
<li>创建临时函数与开发好的java class关联<pre><code>Hive&gt;create temporary function jsonParse as ‘cn.holley.udf.JsonParse&apos;;
</code></pre></li>
<li>在hql中使用自定义的函数parseJson<pre><code>select split( jsonParse(line),’\t’)\[0] as movie, split( jsonParse(line),’\\t’)[1] as rate, split( jsonParse(line),’\t’)[2] as timeStamp, split( jsonParse(line),’\t’)[3] as uid from t_json;
</code></pre></li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hive——级联求和accumulate]]></title>
      <url>http://spark8.tech/2015/10/18/hivejilian/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>有如下访客访问次数统计表 t_access_times<br><img src="/images/hiveacc.png" alt="" title="hiveacc"><br>需要输出报表：t_access_times_accumulate<br><img src="/images/hiveacce.png" alt="" title="hiveacce"></p>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><ol>
<li>自己和自己join</li>
<li>join后记录中取B表月份小于或者等于A表的月份薪酬和<br>此思路适合大部分级联求和案例。</li>
</ol>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><pre><code>select A.username,A.month,max(A.salary) as salary,sum(B.salary) as accumulate
  from 
    (select username,month,sum(salary) as salary from t_access_times group by username,month) A 
    inner join 
    (select username,month,sum(salary) as salary from t_access_times group by username,month) B
    on
    A.username=B.username
    where B.month &lt;= A.month
    group by A.username,A.month
    order by A.username,A.month;
</code></pre>]]></content>
    </entry>
    
  
  
</search>
